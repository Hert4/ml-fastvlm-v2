{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Belle-VLM Inference\n",
        "\n",
        "Vietnamese Vision Language Model - Load từ HuggingFace\n",
        "\n",
        "**Không cần clone repo!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers>=4.51.0 torch torchvision pillow accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torchvision import transforms\n",
        "\n",
        "MODEL_ID = \"beyoru/Belle-VLM\"\n",
        "\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "\n",
        "# Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image processing & inference function\n",
        "IMAGE_TOKEN_INDEX = -200\n",
        "\n",
        "def process_image(image, size=384):\n",
        "    if isinstance(image, str):\n",
        "        image = Image.open(image)\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    \n",
        "    # Pad to square\n",
        "    w, h = image.size\n",
        "    if w != h:\n",
        "        new_size = max(w, h)\n",
        "        new_img = Image.new('RGB', (new_size, new_size), (128, 128, 128))\n",
        "        new_img.paste(image, ((new_size - w) // 2, (new_size - h) // 2))\n",
        "        image = new_img\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.48145466, 0.4578275, 0.40821073],\n",
        "                           [0.26862954, 0.26130258, 0.27577711]),\n",
        "    ])\n",
        "    return transform(image)\n",
        "\n",
        "\n",
        "def tokenize_with_image(prompt, tokenizer):\n",
        "    chunks = prompt.split(\"<image>\")\n",
        "    tokens = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        tokens.extend(tokenizer.encode(chunk, add_special_tokens=(i == 0)))\n",
        "        if i < len(chunks) - 1:\n",
        "            tokens.append(IMAGE_TOKEN_INDEX)\n",
        "    return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "\n",
        "def ask_vlm(image, question, max_tokens=512, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Hỏi Belle-VLM về hình ảnh.\n",
        "    \n",
        "    Args:\n",
        "        image: PIL Image hoặc đường dẫn file\n",
        "        question: Câu hỏi (tiếng Việt hoặc Anh)\n",
        "        max_tokens: Độ dài tối đa câu trả lời\n",
        "        temperature: 0 = chính xác, cao hơn = sáng tạo hơn\n",
        "    \"\"\"\n",
        "    if isinstance(image, str):\n",
        "        pil_image = Image.open(image)\n",
        "    else:\n",
        "        pil_image = image\n",
        "    \n",
        "    image_tensor = process_image(pil_image)\n",
        "    \n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "<image>\n",
        "{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "    \n",
        "    input_ids = tokenize_with_image(prompt, tokenizer).unsqueeze(0).to(model.device)\n",
        "    \n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            images=image_tensor.unsqueeze(0).to(torch.float16, model.device),\n",
        "            image_sizes=[pil_image.size],\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=temperature > 0,\n",
        "            temperature=temperature if temperature > 0 else None,\n",
        "            top_p=0.8 if temperature > 0 else None,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"assistant\" in response:\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"ask_vlm() ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test với dataset\n",
        "from datasets import load_dataset\n",
        "from IPython.display import display\n",
        "\n",
        "dataset = load_dataset(\"5CD-AI/Viet-multimodal-open-r1-8k-verified\", split=\"train\")\n",
        "\n",
        "# Lấy sample\n",
        "test_image = dataset[0]['image']\n",
        "test_question = dataset[0]['vi_problem']\n",
        "\n",
        "display(test_image.resize((300, 300)))\n",
        "print(f\"Question: {test_question[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference\n",
        "response = ask_vlm(test_image, test_question)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"Q: {test_question[:300]}...\")\n",
        "print(f\"\\nA: {response}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test với câu hỏi đơn giản\n",
        "response = ask_vlm(test_image, \"Mô tả hình ảnh này bằng tiếng Việt.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test với URL\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_url(url):\n",
        "    return Image.open(BytesIO(requests.get(url).content))\n",
        "\n",
        "# Uncomment để test:\n",
        "# img = load_url(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/300px-PNG_transparency_demonstration_1.png\")\n",
        "# display(img)\n",
        "# print(ask_vlm(img, \"Đây là gì?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cách dùng\n",
        "\n",
        "```python\n",
        "# Cơ bản\n",
        "response = ask_vlm(image, \"Mô tả hình ảnh này.\")\n",
        "\n",
        "# Từ file\n",
        "response = ask_vlm(\"/path/to/image.jpg\", \"Trong hình có gì?\")\n",
        "\n",
        "# Tùy chỉnh\n",
        "response = ask_vlm(\n",
        "    image,\n",
        "    \"Giải bài toán trong hình.\",\n",
        "    max_tokens=1024,   # Câu trả lời dài hơn\n",
        "    temperature=0.3    # Chính xác hơn\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
