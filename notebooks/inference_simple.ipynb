{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Belle-VLM Inference - Simple Version\n",
        "\n",
        "Notebook inference cho Belle-VLM từ HuggingFace.\n",
        "\n",
        "**Cần clone repo một lần để lấy code model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 1: Install & Clone\n",
        "# ============================================\n",
        "\n",
        "!pip install -q transformers>=4.51.0 torch torchvision\n",
        "!pip install -q accelerate pillow einops timm peft\n",
        "!pip install -q huggingface_hub datasets\n",
        "\n",
        "# Clone repo (chỉ cần làm 1 lần)\n",
        "import os\n",
        "if not os.path.exists('ml-fastvlm-v2'):\n",
        "    !git clone --depth 1 https://github.com/Hert4/ml-fastvlm-v2.git\n",
        "    print(\"Cloned ml-fastvlm-v2\")\n",
        "else:\n",
        "    print(\"ml-fastvlm-v2 already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 2: Setup paths\n",
        "# ============================================\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'ml-fastvlm-v2')\n",
        "\n",
        "# Config\n",
        "HF_MODEL_ID = \"beyoru/Belle-VLM\"\n",
        "\n",
        "print(f\"Model: {HF_MODEL_ID}\")\n",
        "print(\"Path setup done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 3: Load Model\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "from llava.model.builder import load_pretrained_model\n",
        "\n",
        "print(f\"Loading model from {HF_MODEL_ID}...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Load model với builder (handles everything)\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path=HF_MODEL_ID,\n",
        "    model_base=None,\n",
        "    model_name=\"llava-qwen3\",  # MUST contain 'llava'\n",
        "    device_map=\"auto\",\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\nModel loaded!\")\n",
        "print(f\"Image processor: {type(image_processor).__name__ if image_processor else 'None'}\")\n",
        "print(f\"Context length: {context_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 4: Fallback for image_processor\n",
        "# ============================================\n",
        "\n",
        "# If image_processor is None, load manually\n",
        "if image_processor is None:\n",
        "    print(\"image_processor is None, loading fallback...\")\n",
        "    from transformers import CLIPImageProcessor\n",
        "    image_processor = CLIPImageProcessor.from_pretrained(\n",
        "        \"openai/clip-vit-large-patch14-336\"\n",
        "    )\n",
        "    # Adjust for mobileclip_l_384 size\n",
        "    image_processor.size = {\"shortest_edge\": 384}\n",
        "    image_processor.crop_size = {\"height\": 384, \"width\": 384}\n",
        "    print(\"Fallback image_processor loaded!\")\n",
        "else:\n",
        "    print(f\"image_processor OK: {type(image_processor).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 5: Define inference function\n",
        "# ============================================\n",
        "\n",
        "from PIL import Image\n",
        "from llava.conversation import conv_templates\n",
        "from llava.mm_utils import tokenizer_image_token, process_images\n",
        "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
        "\n",
        "def ask_vlm(image, question, max_tokens=512, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Ask Belle-VLM a question about an image.\n",
        "    \n",
        "    Args:\n",
        "        image: PIL Image or path to image\n",
        "        question: Question in Vietnamese or English\n",
        "        max_tokens: Max response length\n",
        "        temperature: Sampling temperature (0 = deterministic)\n",
        "    \n",
        "    Returns:\n",
        "        Response string\n",
        "    \"\"\"\n",
        "    # Load image if path\n",
        "    if isinstance(image, str):\n",
        "        image = Image.open(image)\n",
        "    \n",
        "    # Convert to RGB\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    \n",
        "    # Process image\n",
        "    image_tensor = process_images([image], image_processor, model.config)[0]\n",
        "    \n",
        "    # Build prompt with Qwen3 template\n",
        "    conv = conv_templates[\"qwen_3\"].copy()\n",
        "    conv.append_message(conv.roles[0], f\"{DEFAULT_IMAGE_TOKEN}\\n{question}\")\n",
        "    conv.append_message(conv.roles[1], None)\n",
        "    prompt = conv.get_prompt()\n",
        "    \n",
        "    # Tokenize\n",
        "    input_ids = tokenizer_image_token(\n",
        "        prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n",
        "    ).unsqueeze(0).to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            images=image_tensor.unsqueeze(0).to(dtype=torch.float16, device=model.device),\n",
        "            image_sizes=[image.size],\n",
        "            do_sample=temperature > 0,\n",
        "            temperature=temperature if temperature > 0 else None,\n",
        "            top_p=0.8 if temperature > 0 else None,\n",
        "            max_new_tokens=max_tokens,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
        "    \n",
        "    # Extract assistant response\n",
        "    if \"assistant\\n\" in response:\n",
        "        response = response.split(\"assistant\\n\")[-1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"ask_vlm() function ready!\")\n",
        "print(\"Usage: response = ask_vlm(image, 'Mo ta hinh anh')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 6: Test with dataset\n",
        "# ============================================\n",
        "\n",
        "from datasets import load_dataset\n",
        "from IPython.display import display\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading test dataset...\")\n",
        "dataset = load_dataset(\"5CD-AI/Viet-multimodal-open-r1-8k-verified\", split=\"train\")\n",
        "\n",
        "# Get sample\n",
        "idx = 0\n",
        "test_image = dataset[idx]['image']\n",
        "test_question = dataset[idx]['vi_problem']\n",
        "\n",
        "# Display\n",
        "print(\"Test image:\")\n",
        "display(test_image.resize((300, 300)))\n",
        "print(f\"\\nQuestion: {test_question[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# STEP 7: Run inference\n",
        "# ============================================\n",
        "\n",
        "print(\"Generating response...\\n\")\n",
        "\n",
        "response = ask_vlm(test_image, test_question)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"RESULT\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nQ: {test_question[:300]}...\")\n",
        "print(f\"\\nA: {response}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Test more samples\n",
        "# ============================================\n",
        "\n",
        "# Test voi cau hoi don gian\n",
        "response = ask_vlm(test_image, \"Mo ta hinh anh nay bang tieng Viet.\")\n",
        "print(\"Simple description:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Test with URL image\n",
        "# ============================================\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_image_url(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "# Example (uncomment to use)\n",
        "# url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Oryctolagus_cuniculus_Rcdo.jpg/1200px-Oryctolagus_cuniculus_Rcdo.jpg\"\n",
        "# img = load_image_url(url)\n",
        "# display(img.resize((300, 300)))\n",
        "# response = ask_vlm(img, \"Day la con gi?\")\n",
        "# print(response)\n",
        "\n",
        "print(\"Ready for URL images! Uncomment code above to test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Summary\n",
        "\n",
        "```python\n",
        "# Basic\n",
        "response = ask_vlm(image, \"Mo ta hinh anh nay.\")\n",
        "\n",
        "# From file\n",
        "response = ask_vlm(\"/path/to/image.jpg\", \"Trong hinh co gi?\")\n",
        "\n",
        "# Custom params\n",
        "response = ask_vlm(image, \"Giai bai toan.\", max_tokens=1024, temperature=0.3)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
