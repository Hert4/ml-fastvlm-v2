{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Belle-VLM Inference (Standalone)\n",
        "\n",
        "Notebook độc lập để chạy inference với Belle-VLM từ HuggingFace.\n",
        "\n",
        "**Không cần clone repo GitHub!**\n",
        "\n",
        "## Requirements\n",
        "- Python 3.10+\n",
        "- CUDA GPU (recommended)\n",
        "- ~8GB VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers>=4.51.0 torch torchvision\n",
        "!pip install -q accelerate pillow einops timm\n",
        "!pip install -q huggingface_hub datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIG\n",
        "# ============================================\n",
        "\n",
        "HF_MODEL_ID = \"beyoru/Belle-VLM\"  # Your model on HuggingFace\n",
        "\n",
        "# Image size used during training\n",
        "IMAGE_SIZE = 384\n",
        "\n",
        "print(f\"Model: {HF_MODEL_ID}\")\n",
        "print(f\"Image size: {IMAGE_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONSTANTS (from LLaVA)\n",
        "# ============================================\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "IMAGE_TOKEN_INDEX = -200\n",
        "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
        "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
        "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
        "DEFAULT_IM_END_TOKEN = \"<im_end>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONVERSATION TEMPLATE (Qwen3)\n",
        "# ============================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class Conversation:\n",
        "    \"\"\"Conversation template for Qwen3.\"\"\"\n",
        "    system: str\n",
        "    roles: Tuple[str, str]\n",
        "    messages: List[List[str]]\n",
        "    sep: str\n",
        "    sep2: Optional[str] = None\n",
        "    \n",
        "    def copy(self):\n",
        "        return Conversation(\n",
        "            system=self.system,\n",
        "            roles=self.roles,\n",
        "            messages=[[x, y] for x, y in self.messages],\n",
        "            sep=self.sep,\n",
        "            sep2=self.sep2,\n",
        "        )\n",
        "    \n",
        "    def append_message(self, role: str, message: str):\n",
        "        self.messages.append([role, message])\n",
        "    \n",
        "    def get_prompt(self) -> str:\n",
        "        \"\"\"Build prompt for Qwen3 format.\"\"\"\n",
        "        ret = \"\"\n",
        "        \n",
        "        # Add system message if exists\n",
        "        if self.system:\n",
        "            ret += f\"<|im_start|>system\\n{self.system}<|im_end|>\\n\"\n",
        "        \n",
        "        # Add conversation messages\n",
        "        for role, message in self.messages:\n",
        "            if message:\n",
        "                ret += f\"<|im_start|>{role}\\n{message}<|im_end|>\\n\"\n",
        "            else:\n",
        "                ret += f\"<|im_start|>{role}\\n\"\n",
        "        \n",
        "        return ret\n",
        "\n",
        "\n",
        "# Qwen3 conversation template\n",
        "CONV_QWEN3 = Conversation(\n",
        "    system=\"You are a helpful assistant.\",\n",
        "    roles=(\"user\", \"assistant\"),\n",
        "    messages=[],\n",
        "    sep=\"<|im_end|>\",\n",
        ")\n",
        "\n",
        "print(\"Conversation template ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TOKENIZER UTILS\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "\n",
        "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
        "    \"\"\"\n",
        "    Tokenize prompt with image token handling.\n",
        "    Replaces <image> with IMAGE_TOKEN_INDEX.\n",
        "    \"\"\"\n",
        "    prompt_chunks = prompt.split(DEFAULT_IMAGE_TOKEN)\n",
        "    \n",
        "    # Tokenize each chunk\n",
        "    token_chunks = []\n",
        "    for i, chunk in enumerate(prompt_chunks):\n",
        "        chunk_ids = tokenizer.encode(chunk, add_special_tokens=(i == 0))\n",
        "        token_chunks.append(chunk_ids)\n",
        "    \n",
        "    # Merge with image token\n",
        "    input_ids = []\n",
        "    for i, chunk_ids in enumerate(token_chunks):\n",
        "        input_ids.extend(chunk_ids)\n",
        "        if i < len(token_chunks) - 1:  # Add image token between chunks\n",
        "            input_ids.append(image_token_index)\n",
        "    \n",
        "    if return_tensors == 'pt':\n",
        "        return torch.tensor(input_ids, dtype=torch.long)\n",
        "    return input_ids\n",
        "\n",
        "print(\"Tokenizer utils ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# IMAGE PROCESSING\n",
        "# ============================================\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    \"\"\"Expand image to square with padding.\"\"\"\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width - height) // 2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, ((height - width) // 2, 0))\n",
        "        return result\n",
        "\n",
        "\n",
        "def process_image(image, image_size=384):\n",
        "    \"\"\"\n",
        "    Process image for model input.\n",
        "    \n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        image_size: Target size (default 384 for mobileclip_l_384)\n",
        "    \n",
        "    Returns:\n",
        "        Tensor of shape (3, image_size, image_size)\n",
        "    \"\"\"\n",
        "    # Convert to RGB\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    \n",
        "    # Expand to square with gray padding\n",
        "    image = expand2square(image, (128, 128, 128))\n",
        "    \n",
        "    # Define transforms (CLIP normalization)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP mean\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]   # CLIP std\n",
        "        ),\n",
        "    ])\n",
        "    \n",
        "    return transform(image)\n",
        "\n",
        "print(\"Image processing ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# LOAD MODEL & TOKENIZER\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "print(f\"Loading model from: {HF_MODEL_ID}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    HF_MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded! Vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    HF_MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded!\")\n",
        "print(f\"Model type: {model.config.model_type}\")\n",
        "print(f\"Device: {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SETUP VISION TOWER\n",
        "# ============================================\n",
        "\n",
        "# Check if model has vision tower\n",
        "has_vision_tower = hasattr(model, 'get_vision_tower')\n",
        "print(f\"Has vision tower method: {has_vision_tower}\")\n",
        "\n",
        "if has_vision_tower:\n",
        "    vision_tower = model.get_vision_tower()\n",
        "    \n",
        "    if vision_tower is not None:\n",
        "        if not vision_tower.is_loaded:\n",
        "            print(\"Loading vision tower...\")\n",
        "            vision_tower.load_model()\n",
        "        \n",
        "        vision_tower = vision_tower.to(device=model.device, dtype=torch.float16)\n",
        "        print(f\"Vision tower ready: {type(vision_tower).__name__}\")\n",
        "    else:\n",
        "        print(\"Vision tower is None - model may be LLM-only\")\n",
        "else:\n",
        "    print(\"Model doesn't have vision tower - using as LLM-only\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# INFERENCE FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def ask_vlm(image, question, max_tokens=512, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Ask Belle-VLM a question about an image.\n",
        "    \n",
        "    Args:\n",
        "        image: PIL Image or path to image file\n",
        "        question: Question about the image (Vietnamese or English)\n",
        "        max_tokens: Maximum tokens to generate (default 512)\n",
        "        temperature: Sampling temperature (default 0.7, use 0 for greedy)\n",
        "    \n",
        "    Returns:\n",
        "        Model response as string\n",
        "    \n",
        "    Example:\n",
        "        response = ask_vlm(\"image.jpg\", \"Mo ta hinh anh nay.\")\n",
        "        print(response)\n",
        "    \"\"\"\n",
        "    # Load image if path\n",
        "    if isinstance(image, str):\n",
        "        image = Image.open(image)\n",
        "    \n",
        "    # Process image\n",
        "    image_tensor = process_image(image, IMAGE_SIZE)\n",
        "    \n",
        "    # Build prompt\n",
        "    conv = CONV_QWEN3.copy()\n",
        "    conv.append_message(conv.roles[0], f\"{DEFAULT_IMAGE_TOKEN}\\n{question}\")\n",
        "    conv.append_message(conv.roles[1], None)\n",
        "    prompt = conv.get_prompt()\n",
        "    \n",
        "    # Tokenize\n",
        "    input_ids = tokenizer_image_token(\n",
        "        prompt, \n",
        "        tokenizer, \n",
        "        IMAGE_TOKEN_INDEX, \n",
        "        return_tensors='pt'\n",
        "    ).unsqueeze(0).to(model.device)\n",
        "    \n",
        "    # Prepare image tensor\n",
        "    images = image_tensor.unsqueeze(0).to(dtype=torch.float16, device=model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            images=images,\n",
        "            image_sizes=[image.size],\n",
        "            do_sample=temperature > 0,\n",
        "            temperature=temperature if temperature > 0 else None,\n",
        "            top_p=0.8 if temperature > 0 else None,\n",
        "            max_new_tokens=max_tokens,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
        "    \n",
        "    # Extract assistant response (after last \"assistant\\n\")\n",
        "    if \"assistant\\n\" in response:\n",
        "        response = response.split(\"assistant\\n\")[-1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"Inference function ready!\")\n",
        "print(\"Usage: response = ask_vlm(image, question)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with Sample Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TEST WITH DATASET SAMPLE\n",
        "# ============================================\n",
        "\n",
        "from datasets import load_dataset\n",
        "from IPython.display import display\n",
        "\n",
        "# Load test dataset\n",
        "print(\"Loading test dataset...\")\n",
        "dataset = load_dataset(\"5CD-AI/Viet-multimodal-open-r1-8k-verified\", split=\"train\")\n",
        "\n",
        "# Get sample\n",
        "sample_idx = 0\n",
        "test_image = dataset[sample_idx]['image']\n",
        "test_question = dataset[sample_idx]['vi_problem']\n",
        "expected_answer = dataset[sample_idx]['vi_solution']\n",
        "\n",
        "# Display image\n",
        "print(\"Test Image:\")\n",
        "display(test_image.resize((400, 400)))\n",
        "\n",
        "print(f\"\\nQuestion: {test_question[:300]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# RUN INFERENCE\n",
        "# ============================================\n",
        "\n",
        "print(\"Generating response...\")\n",
        "response = ask_vlm(test_image, test_question)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INFERENCE RESULT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nQuestion:\\n{test_question[:500]}\")\n",
        "print(f\"\\nModel Response:\\n{response}\")\n",
        "print(f\"\\nExpected Answer:\\n{expected_answer[:500]}...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TEST WITH CUSTOM IMAGE\n",
        "# ============================================\n",
        "\n",
        "# Option 1: From URL\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "# Example with URL (uncomment to use)\n",
        "# image_url = \"https://example.com/image.jpg\"\n",
        "# custom_image = load_image_from_url(image_url)\n",
        "# response = ask_vlm(custom_image, \"Mo ta hinh anh nay.\")\n",
        "# print(response)\n",
        "\n",
        "# Option 2: From file path\n",
        "# custom_image = Image.open(\"/path/to/your/image.jpg\")\n",
        "# response = ask_vlm(custom_image, \"Trong hinh co gi?\")\n",
        "# print(response)\n",
        "\n",
        "print(\"Ready to test with custom images!\")\n",
        "print(\"Uncomment the code above to try with your own images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# BATCH INFERENCE (Multiple samples)\n",
        "# ============================================\n",
        "\n",
        "print(\"Testing on multiple samples...\\n\")\n",
        "\n",
        "for i in range(min(3, len(dataset))):\n",
        "    sample = dataset[i]\n",
        "    image = sample['image']\n",
        "    question = sample['vi_problem']\n",
        "    \n",
        "    print(f\"--- Sample {i+1} ---\")\n",
        "    print(f\"Q: {question[:150]}...\")\n",
        "    \n",
        "    response = ask_vlm(image, question, max_tokens=256)\n",
        "    print(f\"A: {response[:300]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Summary\n",
        "\n",
        "```python\n",
        "# Basic usage\n",
        "response = ask_vlm(image, \"Mo ta hinh anh nay.\")\n",
        "\n",
        "# With file path\n",
        "response = ask_vlm(\"/path/to/image.jpg\", \"Trong hinh co gi?\")\n",
        "\n",
        "# With custom parameters\n",
        "response = ask_vlm(\n",
        "    image,\n",
        "    \"Giai bai toan trong hinh.\",\n",
        "    max_tokens=1024,  # Longer response\n",
        "    temperature=0.3   # More focused/deterministic\n",
        ")\n",
        "```\n",
        "\n",
        "### Parameters\n",
        "- `image`: PIL Image or path to image file\n",
        "- `question`: Your question (Vietnamese or English)\n",
        "- `max_tokens`: Maximum response length (default: 512)\n",
        "- `temperature`: Creativity (0 = deterministic, 1 = creative, default: 0.7)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
