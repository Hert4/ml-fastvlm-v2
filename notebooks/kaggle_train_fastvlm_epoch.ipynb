{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastVLM Training with Qwen3-0.6B (EPOCH-based)\n",
    "\n",
    "This notebook trains FastVLM using **EPOCHS** instead of steps.\n",
    "\n",
    "- **LLM Backbone**: Qwen3-0.6B\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP)\n",
    "- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified\n",
    "- **Output**: Push to HuggingFace: beyoru/Belle-VLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.51.0,<5.0.0\n",
    "!pip install -q torch>=2.1.0 torchvision>=0.16.0\n",
    "!pip install -q accelerate>=0.26.0 peft>=0.10.0\n",
    "!pip install -q bitsandbytes>=0.43.0\n",
    "!pip install -q datasets pillow einops timm>=0.9.0\n",
    "!pip install -q deepspeed sentencepiece\n",
    "!pip install -q huggingface_hub safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the FastVLM repository\n",
    "!git clone https://github.com/Hert4/ml-fastvlm-v2.git\n",
    "%cd ml-fastvlm-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration (EPOCH-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - EPOCH BASED\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"llm_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"vision_tower\": \"mobileclip_l_384\",\n",
    "    \n",
    "    # Dataset\n",
    "    \"dataset_name\": \"5CD-AI/Viet-multimodal-open-r1-8k-verified\",\n",
    "    \"image_column\": \"image\",\n",
    "    \"question_column\": \"vi_problem\",\n",
    "    \"answer_column\": \"vi_solution\",\n",
    "    \n",
    "    # Training - EPOCHS\n",
    "    \"output_dir\": \"./outputs/fastvlm-qwen3-0.6b-vietnamese\",\n",
    "    \"num_train_epochs\": 3,              # <-- Number of epochs\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.03,               # <-- 3% of total steps for warmup\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"bf16\": True,\n",
    "    \"model_max_length\": 2048,\n",
    "    \n",
    "    # LoRA\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Save checkpoints - EPOCH based\n",
    "    \"save_strategy\": \"epoch\",           # <-- Save every epoch\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # HuggingFace\n",
    "    \"hf_repo\": \"beyoru/Belle-VLM\",\n",
    "    \"hf_token\": os.environ.get(\"HF_TOKEN\", \"\"),\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration (EPOCH-based):\")\n",
    "for k, v in CONFIG.items():\n",
    "    if k != \"hf_token\":\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# Estimate training\n",
    "effective_batch = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
    "print(f\"\\n Training info:\")\n",
    "print(f\"  Effective batch size: {effective_batch}\")\n",
    "print(f\"  Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(f\"  Warmup: {CONFIG['warmup_ratio']*100:.0f}% of total steps\")\n",
    "print(f\"  Save: every epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "    print(\"Logged in to HuggingFace!\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not set. Set it in Kaggle Secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Columns: {dataset.column_names}\")\n",
    "\n",
    "# Estimate steps per epoch\n",
    "effective_batch = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
    "steps_per_epoch = len(dataset) // effective_batch\n",
    "total_steps = steps_per_epoch * CONFIG[\"num_train_epochs\"]\n",
    "warmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "print(f\"\\n Training estimates:\")\n",
    "print(f\"  Steps per epoch: ~{steps_per_epoch}\")\n",
    "print(f\"  Total steps: ~{total_steps}\")\n",
    "print(f\"  Warmup steps: ~{warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample image\n",
    "from IPython.display import display\n",
    "\n",
    "sample = dataset[0]\n",
    "sample_image = sample['image']\n",
    "if isinstance(sample_image, Image.Image):\n",
    "    display(sample_image.resize((400, 400)))\n",
    "print(f\"\\nQuestion: {sample['vi_problem'][:200]}...\")\n",
    "print(f\"Answer: {sample['vi_solution'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_dataset_to_llava_format(dataset, output_dir, image_folder):\n",
    "    \"\"\"Convert dataset to LLaVA training format.\"\"\"\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    llava_data = []\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=\"Converting dataset\")):\n",
    "        # Save image\n",
    "        image_filename = f\"{idx:06d}.jpg\"\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        \n",
    "        img = sample['image']\n",
    "        if isinstance(img, Image.Image):\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img.save(image_path, 'JPEG', quality=95)\n",
    "        \n",
    "        # Create conversation\n",
    "        question = sample['vi_problem'].strip()\n",
    "        answer = sample['vi_solution'].strip()\n",
    "        \n",
    "        # Truncate very long solutions\n",
    "        if len(answer) > 4096:\n",
    "            answer = answer[:4096] + \"...\"\n",
    "        \n",
    "        llava_sample = {\n",
    "            \"id\": str(sample.get('id', idx)),\n",
    "            \"image\": image_filename,\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": f\"<image>\\n{question}\"},\n",
    "                {\"from\": \"gpt\", \"value\": answer}\n",
    "            ]\n",
    "        }\n",
    "        llava_data.append(llava_sample)\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = os.path.join(output_dir, \"train_data.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(llava_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nDataset converted: {len(llava_data)} samples\")\n",
    "    return json_path, image_folder\n",
    "\n",
    "# Convert dataset\n",
    "DATA_DIR = \"./data\"\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, \"images\")\n",
    "\n",
    "json_path, image_folder = convert_dataset_to_llava_format(dataset, DATA_DIR, IMAGE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model (EPOCH-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training using EPOCHS\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check TF32 support\n",
    "def supports_tf32():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.get_device_capability()[0] >= 8\n",
    "    return False\n",
    "\n",
    "use_tf32 = supports_tf32()\n",
    "\n",
    "# Build command - EPOCH BASED\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"llava.train.train_qwen\",\n",
    "    f\"--model_name_or_path={CONFIG['llm_model']}\",\n",
    "    \"--version=qwen_3\",\n",
    "    f\"--data_path={json_path}\",\n",
    "    f\"--image_folder={image_folder}\",\n",
    "    f\"--vision_tower={CONFIG['vision_tower']}\",\n",
    "    \"--mm_projector_type=mlp2x_gelu\",\n",
    "    \"--mm_vision_select_layer=-2\",\n",
    "    \"--mm_use_im_start_end=False\",\n",
    "    \"--mm_use_im_patch_token=False\",\n",
    "    \"--image_aspect_ratio=pad\",\n",
    "    f\"--output_dir={CONFIG['output_dir']}\",\n",
    "    \n",
    "    # EPOCH CONFIG\n",
    "    f\"--num_train_epochs={CONFIG['num_train_epochs']}\",\n",
    "    f\"--warmup_ratio={CONFIG['warmup_ratio']}\",\n",
    "    f\"--save_strategy={CONFIG['save_strategy']}\",\n",
    "    f\"--save_total_limit={CONFIG['save_total_limit']}\",\n",
    "    \n",
    "    f\"--per_device_train_batch_size={CONFIG['per_device_train_batch_size']}\",\n",
    "    f\"--gradient_accumulation_steps={CONFIG['gradient_accumulation_steps']}\",\n",
    "    f\"--learning_rate={CONFIG['learning_rate']}\",\n",
    "    \"--weight_decay=0.0\",\n",
    "    f\"--lr_scheduler_type={CONFIG['lr_scheduler_type']}\",\n",
    "    \"--logging_steps=10\",\n",
    "    f\"--model_max_length={CONFIG['model_max_length']}\",\n",
    "    \"--gradient_checkpointing=True\",\n",
    "    \"--dataloader_num_workers=4\",\n",
    "    \"--lazy_preprocess=True\",\n",
    "    \"--bf16=True\",\n",
    "    \"--report_to=none\",\n",
    "    \n",
    "    # LoRA\n",
    "    \"--lora_enable=True\",\n",
    "    f\"--lora_r={CONFIG['lora_r']}\",\n",
    "    f\"--lora_alpha={CONFIG['lora_alpha']}\",\n",
    "    f\"--lora_dropout={CONFIG['lora_dropout']}\",\n",
    "]\n",
    "\n",
    "if use_tf32:\n",
    "    cmd.append(\"--tf32=True\")\n",
    "else:\n",
    "    cmd.append(\"--tf32=False\")\n",
    "\n",
    "print(f\"Starting training for {CONFIG['num_train_epochs']} epochs...\")\n",
    "print(f\"Checkpoints will be saved every epoch\")\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "print(f\"\\n Training completed with return code: {process.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Training Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training output\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = CONFIG[\"output_dir\"]\n",
    "print(f\"Checking: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Check for checkpoint folders\n",
    "checkpoints = []\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    for item in os.listdir(OUTPUT_DIR):\n",
    "        item_path = os.path.join(OUTPUT_DIR, item)\n",
    "        if os.path.isdir(item_path) and \"checkpoint\" in item:\n",
    "            checkpoints.append(item_path)\n",
    "\n",
    "# List all files\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        size = os.path.getsize(path) / 1024 / 1024\n",
    "        all_files.append((path, size))\n",
    "        print(f\"  {path} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal files: {len(all_files)}\")\n",
    "print(f\"Checkpoints found: {checkpoints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merge LoRA + mm_projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MERGE LORA + MM_PROJECTOR INTO ONE MODEL\n",
    "# ============================================\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = CONFIG[\"output_dir\"]\n",
    "MERGED_DIR = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading base Qwen3 model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"llm_model\"], trust_remote_code=True)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "# Load and merge LoRA\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "\n",
    "print(\"Merging LoRA weights...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Load mm_projector from non_lora_trainables.bin\n",
    "print(\"\\nLoading mm_projector weights...\")\n",
    "non_lora_path = os.path.join(OUTPUT_DIR, \"non_lora_trainables.bin\")\n",
    "non_lora_weights = torch.load(non_lora_path, map_location=\"cpu\")\n",
    "\n",
    "# Get model state dict\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Add mm_projector weights to state dict\n",
    "mm_count = 0\n",
    "for k, v in non_lora_weights.items():\n",
    "    if 'mm_projector' in k:\n",
    "        # Clean key: remove prefixes\n",
    "        clean_key = k\n",
    "        for prefix in ['base_model.model.', 'model.']:\n",
    "            if clean_key.startswith(prefix):\n",
    "                clean_key = clean_key[len(prefix):]\n",
    "        \n",
    "        # Add \"model.\" prefix for LlavaQwen3 structure\n",
    "        if not clean_key.startswith('model.'):\n",
    "            clean_key = 'model.' + clean_key\n",
    "        \n",
    "        state_dict[clean_key] = v.to(torch.float16)\n",
    "        print(f\"  Added: {clean_key} {v.shape}\")\n",
    "        mm_count += 1\n",
    "\n",
    "print(f\"\\nAdded {mm_count} mm_projector tensors\")\n",
    "\n",
    "# Save merged model with mm_projector included\n",
    "print(\"\\nSaving complete model...\")\n",
    "model.save_pretrained(MERGED_DIR, state_dict=state_dict, safe_serialization=True)\n",
    "\n",
    "print(f\"\\n Model saved to: {MERGED_DIR}\")\n",
    "print(\"   mm_projector is NOW INCLUDED in model.safetensors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFY mm_projector IS IN model.safetensors\n",
    "# ============================================\n",
    "from safetensors import safe_open\n",
    "import os\n",
    "\n",
    "safetensor_path = os.path.join(MERGED_DIR, \"model.safetensors\")\n",
    "\n",
    "if os.path.exists(safetensor_path):\n",
    "    print(f\" {safetensor_path}\")\n",
    "    print(f\"   Size: {os.path.getsize(safetensor_path) / 1024 / 1024:.2f} MB\\n\")\n",
    "    \n",
    "    with safe_open(safetensor_path, framework=\"pt\") as f:\n",
    "        keys = f.keys()\n",
    "        \n",
    "        # Find mm_projector keys\n",
    "        mm_keys = [k for k in keys if 'mm_projector' in k]\n",
    "        \n",
    "        if mm_keys:\n",
    "            print(\" mm_projector FOUND in model.safetensors:\")\n",
    "            for k in mm_keys:\n",
    "                tensor = f.get_tensor(k)\n",
    "                print(f\"   {k}: {tensor.shape}\")\n",
    "        else:\n",
    "            print(\" mm_projector NOT FOUND!\")\n",
    "else:\n",
    "    print(f\" File not found: {safetensor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UPDATE CONFIG.JSON\n",
    "# ============================================\n",
    "import json\n",
    "\n",
    "src_config = os.path.join(OUTPUT_DIR, \"config.json\")\n",
    "dst_config = os.path.join(MERGED_DIR, \"config.json\")\n",
    "\n",
    "if os.path.exists(src_config):\n",
    "    with open(src_config, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    # Add auto_map for HuggingFace\n",
    "    config_data[\"auto_map\"] = {\n",
    "        \"AutoConfig\": \"configuration_llava_qwen.LlavaQwen3Config\",\n",
    "        \"AutoModelForCausalLM\": \"modeling_llava_qwen.LlavaQwen3ForCausalLM\"\n",
    "    }\n",
    "    config_data[\"architectures\"] = [\"LlavaQwen3ForCausalLM\"]\n",
    "    \n",
    "    with open(dst_config, 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    print(\" Updated config.json with auto_map\")\n",
    "\n",
    "# Verify files\n",
    "print(f\"\\n Files in {MERGED_DIR}:\")\n",
    "for f in sorted(os.listdir(MERGED_DIR)):\n",
    "    size = os.path.getsize(os.path.join(MERGED_DIR, f)) / 1024 / 1024\n",
    "    print(f\"  {f} ({size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card\n",
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- vi\n",
    "- en\n",
    "tags:\n",
    "- vision-language-model\n",
    "- vlm\n",
    "- qwen3\n",
    "- fastvlm\n",
    "- vietnamese\n",
    "base_model: {CONFIG['llm_model']}\n",
    "datasets:\n",
    "- {CONFIG['dataset_name']}\n",
    "---\n",
    "\n",
    "# Belle-VLM: Vietnamese Vision Language Model\n",
    "\n",
    "## Model Description\n",
    "\n",
    "Belle-VLM is a Vision Language Model trained for Vietnamese multimodal reasoning tasks.\n",
    "\n",
    "### Architecture\n",
    "- **LLM Backbone**: Qwen3-0.6B\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP)\n",
    "- **Projector**: MLP 2-layer (3072 -> 1024)\n",
    "\n",
    "### Training\n",
    "- **Dataset**: {CONFIG['dataset_name']}\n",
    "- **Method**: LoRA fine-tuning\n",
    "- **Epochs**: {CONFIG['num_train_epochs']}\n",
    "- **Learning Rate**: {CONFIG['learning_rate']}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{CONFIG['hf_repo']}\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{CONFIG['hf_repo']}\", trust_remote_code=True)\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Base Model | {CONFIG['llm_model']} |\n",
    "| Vision Tower | {CONFIG['vision_tower']} |\n",
    "| LoRA Rank | {CONFIG['lora_r']} |\n",
    "| LoRA Alpha | {CONFIG['lora_alpha']} |\n",
    "| Batch Size | {CONFIG['per_device_train_batch_size']} x {CONFIG['gradient_accumulation_steps']} |\n",
    "| Epochs | {CONFIG['num_train_epochs']} |\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0\n",
    "\"\"\"\n",
    "\n",
    "readme_path = os.path.join(MERGED_DIR, \"README.md\")\n",
    "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\" Model card created!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# CREATE CUSTOM CODE FILES FOR HUGGINGFACE\n# ============================================\n# These files are REQUIRED for trust_remote_code=True\n\n# 1. configuration_llava_qwen.py\nconfig_code = '''# Configuration for LLaVA Qwen3 model\nfrom transformers import Qwen2Config\ntry:\n    from transformers import Qwen3Config\n    QWEN3_AVAILABLE = True\nexcept ImportError:\n    QWEN3_AVAILABLE = False\n    Qwen3Config = Qwen2Config\n\nclass LlavaQwen3Config(Qwen3Config if QWEN3_AVAILABLE else Qwen2Config):\n    model_type = \"llava_qwen3\"\n    def __init__(self, mm_vision_tower=None, mm_hidden_size=None, mm_projector_type=\"mlp2x_gelu\",\n                 mm_vision_select_layer=-2, mm_vision_select_feature=\"patch\", mm_patch_merge_type=\"flat\",\n                 mm_use_im_start_end=False, mm_use_im_patch_token=False, image_aspect_ratio=\"pad\", **kwargs):\n        super().__init__(**kwargs)\n        self.mm_vision_tower = mm_vision_tower\n        self.mm_hidden_size = mm_hidden_size\n        self.mm_projector_type = mm_projector_type\n        self.mm_vision_select_layer = mm_vision_select_layer\n        self.mm_vision_select_feature = mm_vision_select_feature\n        self.mm_patch_merge_type = mm_patch_merge_type\n        self.mm_use_im_start_end = mm_use_im_start_end\n        self.mm_use_im_patch_token = mm_use_im_patch_token\n        self.image_aspect_ratio = image_aspect_ratio\n'''\n\nwith open(\"/tmp/configuration_llava_qwen.py\", \"w\") as f:\n    f.write(config_code)\n\nprint(\"Created configuration_llava_qwen.py\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 2. modeling_llava_qwen.py\nmodeling_code = '''# LLaVA Qwen3 Model\nfrom typing import List, Optional, Tuple, Union\nfrom abc import ABC, abstractmethod\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModelForCausalLM, Qwen2Config, Qwen2Model, Qwen2ForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\ntry:\n    from transformers import Qwen3Config, Qwen3Model, Qwen3ForCausalLM\n    QWEN3_AVAILABLE = True\nexcept ImportError:\n    QWEN3_AVAILABLE = False\n    Qwen3Config, Qwen3Model, Qwen3ForCausalLM = Qwen2Config, Qwen2Model, Qwen2ForCausalLM\n\nfrom .configuration_llava_qwen import LlavaQwen3Config\n\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\n\ndef build_vision_projector(config):\n    projector_type = getattr(config, \"mm_projector_type\", \"mlp2x_gelu\")\n    mm_hidden_size = getattr(config, \"mm_hidden_size\", 3072)\n    hidden_size = config.hidden_size\n    if projector_type == \"mlp2x_gelu\":\n        return nn.Sequential(nn.Linear(mm_hidden_size, hidden_size), nn.GELU(), nn.Linear(hidden_size, hidden_size))\n    return nn.Linear(mm_hidden_size, hidden_size)\n\nclass MobileCLIPVisionTower(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.is_loaded = False\n        self.image_processor = None\n        self.vision_tower = None\n        self.hidden_size = getattr(config, \"mm_hidden_size\", 3072)\n        try:\n            self.image_size = int(getattr(config, \"mm_vision_tower\", \"mobileclip_l_384\").split(\"_\")[-1])\n        except:\n            self.image_size = 384\n\n    def load_model(self, **kwargs):\n        if self.is_loaded:\n            return\n        try:\n            import timm\n            from transformers import CLIPImageProcessor\n            self.vision_tower = timm.create_model(\"fastvit_mci2.apple_mclip\", pretrained=True, num_classes=0)\n            self.vision_tower.eval()\n            self.image_processor = CLIPImageProcessor(size={\"shortest_edge\": self.image_size},\n                crop_size={\"height\": self.image_size, \"width\": self.image_size}, do_normalize=True,\n                image_mean=[0.48145466, 0.4578275, 0.40821073], image_std=[0.26862954, 0.26130258, 0.27577711])\n            self.is_loaded = True\n            print(\"MobileCLIP loaded!\")\n        except Exception as e:\n            print(f\"Could not load MobileCLIP: {e}\")\n            self.is_loaded = True\n\n    def forward(self, images):\n        if not self.is_loaded:\n            self.load_model()\n        with torch.no_grad():\n            features = self.vision_tower.forward_features(images) if hasattr(self.vision_tower, \"forward_features\") else self.vision_tower(images)\n        if features.dim() == 4:\n            B, C, H, W = features.shape\n            features = features.flatten(2).transpose(1, 2)\n        elif features.dim() == 2:\n            features = features.unsqueeze(1)\n        return features\n\n    def to(self, *args, **kwargs):\n        super().to(*args, **kwargs)\n        if self.vision_tower is not None:\n            self.vision_tower = self.vision_tower.to(*args, **kwargs)\n        return self\n\nclass LlavaMetaModel:\n    def __init__(self, config):\n        super().__init__(config)\n        if hasattr(config, \"mm_vision_tower\") and config.mm_vision_tower:\n            self.vision_tower = MobileCLIPVisionTower(config)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        return getattr(self, \"vision_tower\", None)\n\nclass LlavaMetaForCausalLM(ABC):\n    @abstractmethod\n    def get_model(self): pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        vt = self.get_model().get_vision_tower()\n        if not vt.is_loaded:\n            vt.load_model()\n            vt = vt.to(device=images.device, dtype=images.dtype)\n        return self.get_model().mm_projector(vt(images))\n\nclass LlavaQwen3Model(LlavaMetaModel, Qwen3Model):\n    config_class = LlavaQwen3Config\n    def __init__(self, config):\n        super().__init__(config)\n\nclass LlavaQwen3ForCausalLM(Qwen3ForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwen3Config\n    def __init__(self, config):\n        super(Qwen3ForCausalLM, self).__init__(config)\n        self.model = LlavaQwen3Model(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(self, input_ids=None, attention_mask=None, position_ids=None, past_key_values=None,\n                inputs_embeds=None, labels=None, use_cache=None, output_attentions=None,\n                output_hidden_states=None, images=None, image_sizes=None, return_dict=None, **kwargs):\n        return super().forward(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids,\n            past_key_values=past_key_values, inputs_embeds=inputs_embeds, labels=labels, use_cache=use_cache,\n            output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)\n\n    @torch.no_grad()\n    def generate(self, inputs=None, images=None, image_sizes=None, **kwargs):\n        if images is not None:\n            inputs_embeds = self.encode_images(images)\n            text_embeds = self.get_model().embed_tokens(inputs)\n            inputs_embeds = torch.cat([inputs_embeds, text_embeds], dim=1)\n            return super().generate(inputs_embeds=inputs_embeds, **kwargs)\n        return super().generate(inputs, **kwargs)\n\nAutoConfig.register(\"llava_qwen3\", LlavaQwen3Config)\nAutoModelForCausalLM.register(LlavaQwen3Config, LlavaQwen3ForCausalLM)\n'''\n\nwith open(\"/tmp/modeling_llava_qwen.py\", \"w\") as f:\n    f.write(modeling_code)\n\nprint(\"Created modeling_llava_qwen.py\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# UPLOAD TO HUGGINGFACE (with custom code)\n# ============================================\nfrom huggingface_hub import HfApi, create_repo\n\napi = HfApi(token=CONFIG[\"hf_token\"])\n\n# Create repo\ntry:\n    create_repo(repo_id=CONFIG[\"hf_repo\"], repo_type=\"model\", exist_ok=True, token=CONFIG[\"hf_token\"])\n    print(f\"Repository ready: {CONFIG['hf_repo']}\")\nexcept Exception as e:\n    print(f\"Repo exists or error: {e}\")\n\n# 1. Upload model files\nprint(f\"\\nUploading model to {CONFIG['hf_repo']}...\")\napi.upload_folder(\n    folder_path=MERGED_DIR,\n    repo_id=CONFIG[\"hf_repo\"],\n    repo_type=\"model\",\n    commit_message=\"Upload Belle-VLM (epoch-based training)\",\n)\nprint(\"Model uploaded!\")\n\n# 2. Upload custom code files (REQUIRED for trust_remote_code)\nprint(\"\\nUploading custom code files...\")\n\napi.upload_file(\n    path_or_fileobj=\"/tmp/configuration_llava_qwen.py\",\n    path_in_repo=\"configuration_llava_qwen.py\",\n    repo_id=CONFIG[\"hf_repo\"],\n)\nprint(\"Uploaded configuration_llava_qwen.py\")\n\napi.upload_file(\n    path_or_fileobj=\"/tmp/modeling_llava_qwen.py\",\n    path_in_repo=\"modeling_llava_qwen.py\",\n    repo_id=CONFIG[\"hf_repo\"],\n)\nprint(\"Uploaded modeling_llava_qwen.py\")\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Model uploaded successfully!\")\nprint(f\"https://huggingface.co/{CONFIG['hf_repo']}\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST INFERENCE\n",
    "# ============================================\n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPImageProcessor\n",
    "\n",
    "sys.path.insert(0, '/kaggle/working/ml-fastvlm-v2')\n",
    "\n",
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.model.language_model.llava_qwen import LlavaQwen3ForCausalLM\n",
    "\n",
    "HF_MODEL_PATH = CONFIG[\"hf_repo\"]\n",
    "\n",
    "print(f\"Loading model from: {HF_MODEL_PATH}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = LlavaQwen3ForCausalLM.from_pretrained(\n",
    "    HF_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Setup vision tower\n",
    "vision_tower = model.get_vision_tower()\n",
    "if not vision_tower.is_loaded:\n",
    "    vision_tower.load_model()\n",
    "vision_tower = vision_tower.to(device=model.device, dtype=torch.float16)\n",
    "image_processor = vision_tower.image_processor\n",
    "\n",
    "print(\" Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with dataset\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "\n",
    "test_dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
    "test_image = test_dataset[0]['image']\n",
    "test_question = test_dataset[0]['vi_problem']\n",
    "\n",
    "display(test_image.resize((300, 300)))\n",
    "print(f\"\\nQuestion: {test_question[:200]}...\")\n",
    "\n",
    "# Process\n",
    "if test_image.mode != 'RGB':\n",
    "    test_image = test_image.convert('RGB')\n",
    "\n",
    "image_tensor = process_images([test_image], image_processor, model.config)[0]\n",
    "\n",
    "conv = conv_templates[\"qwen_3\"].copy()\n",
    "conv.append_message(conv.roles[0], f\"{DEFAULT_IMAGE_TOKEN}\\n{test_question}\")\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "\n",
    "# Generate\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor.unsqueeze(0).to(dtype=torch.float16, device=model.device),\n",
    "        image_sizes=[test_image.size],\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Model trained with **EPOCHS** and uploaded to HuggingFace.\n",
    "\n",
    "View: https://huggingface.co/beyoru/Belle-VLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}