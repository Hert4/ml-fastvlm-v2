{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastVLM Training with Ouro-1.4B (LoopLM)\n",
    "\n",
    "Train FastVLM using **ByteDance Ouro-1.4B** - a Looped Language Model.\n",
    "\n",
    "- **LLM Backbone**: Ouro-1.4B (LoopLM architecture)\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP)\n",
    "- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified\n",
    "\n",
    "## Ouro Architecture\n",
    "- **Hidden Size**: 2048\n",
    "- **Layers**: 24 (x4 recurrent steps)\n",
    "- **Effective capacity**: ~4-12B model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# IMPORTANT: Ouro requires transformers < 4.56.0\n",
    "!pip install -q transformers==4.54.1\n",
    "!pip install -q torch>=2.1.0 torchvision>=0.16.0\n",
    "!pip install -q accelerate>=0.26.0 peft>=0.10.0\n",
    "!pip install -q bitsandbytes>=0.43.0\n",
    "!pip install -q datasets pillow einops timm>=0.9.0\n",
    "!pip install -q sentencepiece safetensors\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify transformers version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "assert transformers.__version__ < \"4.56.0\", \"Ouro requires transformers < 4.56.0!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - OURO-1.4B\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model - OURO\n",
    "    \"llm_model\": \"ByteDance/Ouro-1.4B\",\n",
    "    \"vision_tower\": \"mobileclip_l_384\",\n",
    "    \"mm_hidden_size\": 3072,       # MobileCLIP output (will auto-detect)\n",
    "    \"llm_hidden_size\": 2048,      # Ouro hidden size\n",
    "    \n",
    "    # Dataset\n",
    "    \"dataset_name\": \"5CD-AI/Viet-multimodal-open-r1-8k-verified\",\n",
    "    \"image_column\": \"image\",\n",
    "    \"question_column\": \"vi_problem\",\n",
    "    \"answer_column\": \"vi_solution\",\n",
    "    \n",
    "    # ============================================\n",
    "    # TRAINING MODE: \"steps\" or \"epochs\"\n",
    "    # ============================================\n",
    "    \"training_mode\": \"steps\",  # <-- CHANGE THIS: \"steps\" or \"epochs\"\n",
    "    \n",
    "    # Step-based settings (used if training_mode=\"steps\")\n",
    "    \"max_steps\": 100,              # Total training steps\n",
    "    \"warmup_steps\": 10,            # Warmup steps\n",
    "    \"save_steps\": 50,              # Save every N steps\n",
    "    \n",
    "    # Epoch-based settings (used if training_mode=\"epochs\")\n",
    "    \"num_train_epochs\": 2,         # Number of epochs\n",
    "    \"warmup_ratio\": 0.03,          # Warmup ratio (3% of total)\n",
    "    \"save_strategy\": \"epoch\",      # Save every epoch\n",
    "    \n",
    "    # Common settings\n",
    "    \"output_dir\": \"./outputs/fastvlm-ouro-1.4b-vietnamese\",\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"bf16\": True,\n",
    "    \"model_max_length\": 2048,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # LoRA\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # HuggingFace\n",
    "    \"hf_repo\": \"beyoru/Belle-VLM-Ouro\",\n",
    "    \"hf_token\": os.environ.get(\"HF_TOKEN\", \"\"),\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Print config\n",
    "print(\"=\"*50)\n",
    "print(f\"TRAINING MODE: {CONFIG['training_mode'].upper()}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nConfiguration (Ouro-1.4B):\")\n",
    "for k, v in CONFIG.items():\n",
    "    if k == \"hf_token\":\n",
    "        continue\n",
    "    # Highlight relevant settings based on mode\n",
    "    if CONFIG[\"training_mode\"] == \"steps\":\n",
    "        if k in [\"num_train_epochs\", \"warmup_ratio\", \"save_strategy\"]:\n",
    "            continue  # Skip epoch settings\n",
    "    else:\n",
    "        if k in [\"max_steps\", \"warmup_steps\", \"save_steps\"]:\n",
    "            continue  # Skip step settings\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Estimate\n",
    "effective_batch = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
    "print(f\"\\nEffective batch size: {effective_batch}\")\n",
    "\n",
    "if CONFIG[\"training_mode\"] == \"steps\":\n",
    "    print(f\"Total steps: {CONFIG['max_steps']}\")\n",
    "    print(f\"Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "    print(f\"Save every: {CONFIG['save_steps']} steps\")\n",
    "else:\n",
    "    print(f\"Epochs: {CONFIG['num_train_epochs']}\")\n",
    "    print(f\"Warmup: {CONFIG['warmup_ratio']*100:.0f}% of total\")\n",
    "    print(f\"Save: every epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "    print(\"Logged in to HuggingFace!\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Ouro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "print(f\"Loading Ouro model: {CONFIG['llm_model']}\")\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=CONFIG[\"model_max_length\"],\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Load Ouro model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Total UT steps: {getattr(model.config, 'total_ut_steps', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip - mm_projector will be created AFTER LoRA\n",
    "# (If we create it here, it will be lost after get_peft_model wraps the model)\n",
    "print(\"mm_projector will be created after LoRA setup...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# LoRA config for Ouro\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ============================================\n",
    "# NOW create mm_projector (AFTER LoRA wrap)\n",
    "# ============================================\n",
    "import torch.nn as nn\n",
    "\n",
    "# Build mm_projector: 3072 (MobileCLIP) -> 2048 (Ouro)\n",
    "mm_projector = nn.Sequential(\n",
    "    nn.Linear(CONFIG[\"mm_hidden_size\"], CONFIG[\"llm_hidden_size\"]),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(CONFIG[\"llm_hidden_size\"], CONFIG[\"llm_hidden_size\"]),\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "# Attach to PeftModel\n",
    "model.mm_projector = mm_projector\n",
    "\n",
    "# Make mm_projector trainable\n",
    "for param in model.mm_projector.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"\\nAdded mm_projector: {CONFIG['mm_hidden_size']} -> {CONFIG['llm_hidden_size']}\")\n",
    "proj_params = sum(p.numel() for p in mm_projector.parameters())\n",
    "print(f\"mm_projector parameters: {proj_params / 1e6:.2f}M (trainable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Vision Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "# Load MobileCLIP vision tower\n",
    "print(\"Loading MobileCLIP vision tower...\")\n",
    "vision_tower = timm.create_model(\n",
    "    \"fastvit_mci2.apple_mclip\",\n",
    "    pretrained=True,\n",
    "    num_classes=0,\n",
    ")\n",
    "vision_tower.eval()\n",
    "vision_tower = vision_tower.to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "# Image processor\n",
    "image_processor = CLIPImageProcessor(\n",
    "    size={\"shortest_edge\": 384},\n",
    "    crop_size={\"height\": 384, \"width\": 384},\n",
    "    do_center_crop=True,\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "    image_std=[0.26862954, 0.26130258, 0.27577711],\n",
    ")\n",
    "\n",
    "# Test output dimension\n",
    "dummy_img = torch.randn(1, 3, 384, 384).to(model.device, dtype=torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    features = vision_tower.forward_features(dummy_img)\n",
    "    if features.dim() == 4:\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.flatten(2).transpose(1, 2)\n",
    "        \n",
    "print(f\"Vision tower output: {features.shape}\")\n",
    "actual_hidden = features.shape[-1]\n",
    "print(f\"Actual hidden size: {actual_hidden}\")\n",
    "\n",
    "# Verify mm_projector matches\n",
    "expected_hidden = CONFIG[\"mm_hidden_size\"]\n",
    "if actual_hidden != expected_hidden:\n",
    "    print(f\"WARNING: Config mm_hidden_size={expected_hidden} but vision tower outputs {actual_hidden}\")\n",
    "    print(f\"Updating CONFIG and recreating mm_projector...\")\n",
    "    CONFIG[\"mm_hidden_size\"] = actual_hidden\n",
    "    \n",
    "    # Recreate mm_projector with correct size\n",
    "    model.mm_projector = nn.Sequential(\n",
    "        nn.Linear(actual_hidden, CONFIG[\"llm_hidden_size\"]),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(CONFIG[\"llm_hidden_size\"], CONFIG[\"llm_hidden_size\"]),\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "    \n",
    "    for param in model.mm_projector.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(f\"Recreated mm_projector: {actual_hidden} -> {CONFIG['llm_hidden_size']}\")\n",
    "\n",
    "# Attach to model\n",
    "model.vision_tower = vision_tower\n",
    "model.image_processor = image_processor\n",
    "\n",
    "print(\"Vision tower ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Columns: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLaVA format data\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, \"images\")\n",
    "os.makedirs(IMAGE_FOLDER, exist_ok=True)\n",
    "\n",
    "llava_data = []\n",
    "\n",
    "for idx, sample in enumerate(tqdm(dataset, desc=\"Converting\")):\n",
    "    # Save image\n",
    "    image_filename = f\"{idx:06d}.jpg\"\n",
    "    image_path = os.path.join(IMAGE_FOLDER, image_filename)\n",
    "    \n",
    "    img = sample['image']\n",
    "    if isinstance(img, Image.Image):\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img.save(image_path, 'JPEG', quality=95)\n",
    "    \n",
    "    # Conversation\n",
    "    question = sample['vi_problem'].strip()\n",
    "    answer = sample['vi_solution'].strip()\n",
    "    if len(answer) > 4096:\n",
    "        answer = answer[:4096] + \"...\"\n",
    "    \n",
    "    llava_data.append({\n",
    "        \"id\": str(idx),\n",
    "        \"image\": image_filename,\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": f\"<image>\\n{question}\"},\n",
    "            {\"from\": \"gpt\", \"value\": answer}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Save\n",
    "json_path = os.path.join(DATA_DIR, \"train_data.json\")\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(llava_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Dataset converted: {len(llava_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "\n",
    "class VLMDataset(Dataset):\n",
    "    \"\"\"Simple VLM dataset for training.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, image_folder, tokenizer, image_processor, max_length=2048):\n",
    "        self.data = data\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711]\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_folder, item['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        # Build conversation\n",
    "        conv = item['conversations']\n",
    "        question = conv[0]['value'].replace('<image>', '').strip()\n",
    "        answer = conv[1]['value']\n",
    "        \n",
    "        # Format prompt (Ouro uses standard format)\n",
    "        prompt = f\"User: <image>\\n{question}\\nAssistant: {answer}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Labels (mask prompt, only train on answer)\n",
    "        labels = input_ids.clone()\n",
    "        # Find where answer starts\n",
    "        answer_start = prompt.find('Assistant:') + len('Assistant:')\n",
    "        answer_tokens = self.tokenizer(prompt[:answer_start], return_tensors='pt')['input_ids'].shape[1]\n",
    "        labels[:answer_tokens] = -100  # Ignore prompt tokens\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'images': image_tensor,\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = VLMDataset(\n",
    "    llava_data, \n",
    "    IMAGE_FOLDER, \n",
    "    tokenizer, \n",
    "    image_processor,\n",
    "    max_length=CONFIG[\"model_max_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# ============================================\n",
    "# TRAINING ARGUMENTS - Auto-detect mode from CONFIG\n",
    "# ============================================\n",
    "if CONFIG[\"training_mode\"] == \"steps\":\n",
    "    # STEP-BASED training\n",
    "    print(\"Mode: STEP-BASED training\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=CONFIG[\"output_dir\"],\n",
    "        max_steps=CONFIG[\"max_steps\"],\n",
    "        per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "        lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "        bf16=CONFIG[\"bf16\"],\n",
    "        logging_steps=10,\n",
    "        save_steps=CONFIG[\"save_steps\"],\n",
    "        save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(f\"  max_steps: {CONFIG['max_steps']}\")\n",
    "    print(f\"  warmup_steps: {CONFIG['warmup_steps']}\")\n",
    "    print(f\"  save_steps: {CONFIG['save_steps']}\")\n",
    "else:\n",
    "    # EPOCH-BASED training\n",
    "    print(\"Mode: EPOCH-BASED training\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=CONFIG[\"output_dir\"],\n",
    "        num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "        per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "        lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "        bf16=CONFIG[\"bf16\"],\n",
    "        logging_steps=10,\n",
    "        save_strategy=CONFIG[\"save_strategy\"],\n",
    "        save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(f\"  num_train_epochs: {CONFIG['num_train_epochs']}\")\n",
    "    print(f\"  warmup_ratio: {CONFIG['warmup_ratio']}\")\n",
    "    print(f\"  save_strategy: {CONFIG['save_strategy']}\")\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch]),\n",
    "        'images': torch.stack([x['images'] for x in batch]),\n",
    "    }\n",
    "\n",
    "print(\"\\nTraining arguments ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for multimodal\n",
    "class VLMTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        images = inputs.pop('images').to(model.device, dtype=torch.bfloat16)\n",
    "        input_ids = inputs['input_ids']\n",
    "        labels = inputs['labels']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Encode images\n",
    "        with torch.no_grad():\n",
    "            image_features = model.vision_tower.forward_features(images)\n",
    "            if image_features.dim() == 4:\n",
    "                B, C, H, W = image_features.shape\n",
    "                image_features = image_features.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Project\n",
    "        image_features = model.mm_projector(image_features)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        text_embeds = model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # Concatenate image + text\n",
    "        inputs_embeds = torch.cat([image_features, text_embeds], dim=1)\n",
    "        \n",
    "        # Adjust attention mask\n",
    "        image_mask = torch.ones(\n",
    "            images.size(0), image_features.size(1),\n",
    "            device=attention_mask.device, dtype=attention_mask.dtype\n",
    "        )\n",
    "        attention_mask = torch.cat([image_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # Adjust labels\n",
    "        image_labels = torch.full(\n",
    "            (images.size(0), image_features.size(1)),\n",
    "            -100,\n",
    "            device=labels.device, dtype=labels.dtype\n",
    "        )\n",
    "        labels = torch.cat([image_labels, labels], dim=1)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return (outputs.loss, outputs) if return_outputs else outputs.loss\n",
    "\n",
    "# Create trainer\n",
    "trainer = VLMTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "if CONFIG[\"training_mode\"] == \"steps\":\n",
    "    print(f\"Starting training for {CONFIG['max_steps']} steps...\")\n",
    "else:\n",
    "    print(f\"Starting training for {CONFIG['num_train_epochs']} epochs...\")\n",
    "    \n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(CONFIG[\"output_dir\"])\n",
    "\n",
    "# Save mm_projector separately (from PeftModel)\n",
    "mm_projector_path = os.path.join(CONFIG[\"output_dir\"], \"mm_projector.bin\")\n",
    "\n",
    "# Access mm_projector from the model (works for both PeftModel and regular model)\n",
    "if hasattr(model, 'mm_projector'):\n",
    "    mm_proj = model.mm_projector\n",
    "elif hasattr(model, 'base_model') and hasattr(model.base_model, 'mm_projector'):\n",
    "    mm_proj = model.base_model.mm_projector\n",
    "else:\n",
    "    raise ValueError(\"Cannot find mm_projector in model!\")\n",
    "\n",
    "torch.save(mm_proj.state_dict(), mm_projector_path)\n",
    "print(f\"Saved mm_projector to {mm_projector_path}\")\n",
    "print(f\"  Size: {os.path.getsize(mm_projector_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "OUTPUT_DIR = CONFIG[\"output_dir\"]\n",
    "MERGED_DIR = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading base model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Load mm_projector weights\n",
    "mm_projector_path = os.path.join(OUTPUT_DIR, \"mm_projector.bin\")\n",
    "print(f\"\\nLoading mm_projector from {mm_projector_path}\")\n",
    "mm_projector_weights = torch.load(mm_projector_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"mm_projector weights:\")\n",
    "for k, v in mm_projector_weights.items():\n",
    "    print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "# Get state dict and add mm_projector\n",
    "state_dict = merged_model.state_dict()\n",
    "\n",
    "# Add mm_projector with correct key format\n",
    "for k, v in mm_projector_weights.items():\n",
    "    # Key format: \"model.mm_projector.0.weight\" etc\n",
    "    new_key = f\"model.mm_projector.{k}\"\n",
    "    state_dict[new_key] = v.to(torch.float16)\n",
    "    print(f\"Added: {new_key}\")\n",
    "\n",
    "# Save\n",
    "merged_model.save_pretrained(MERGED_DIR, state_dict=state_dict, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(f\"\\nModel saved to: {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for LLaVA-Ouro\n",
    "import json\n",
    "\n",
    "config_data = merged_model.config.to_dict()\n",
    "config_data[\"model_type\"] = \"llava_ouro\"\n",
    "config_data[\"architectures\"] = [\"LlavaOuroForCausalLM\"]\n",
    "config_data[\"mm_vision_tower\"] = CONFIG[\"vision_tower\"]\n",
    "config_data[\"mm_hidden_size\"] = CONFIG[\"mm_hidden_size\"]\n",
    "config_data[\"mm_projector_type\"] = \"mlp2x_gelu\"\n",
    "config_data[\"auto_map\"] = {\n",
    "    \"AutoConfig\": \"configuration_llava_ouro.LlavaOuroConfig\",\n",
    "    \"AutoModelForCausalLM\": \"modeling_llava_ouro.LlavaOuroForCausalLM\"\n",
    "}\n",
    "\n",
    "config_path = os.path.join(MERGED_DIR, \"config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_data, f, indent=2)\n",
    "\n",
    "print(\"Config saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "from safetensors import safe_open\n",
    "\n",
    "safetensor_path = os.path.join(MERGED_DIR, \"model.safetensors\")\n",
    "print(f\"Model size: {os.path.getsize(safetensor_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "with safe_open(safetensor_path, framework=\"pt\") as f:\n",
    "    mm_keys = [k for k in f.keys() if 'mm_projector' in k]\n",
    "    if mm_keys:\n",
    "        print(\"\\nmm_projector found:\")\n",
    "        for k in mm_keys:\n",
    "            print(f\"  {k}: {f.get_tensor(k).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card\n",
    "if CONFIG[\"training_mode\"] == \"steps\":\n",
    "    training_info = f\"Steps: {CONFIG['max_steps']}\"\n",
    "else:\n",
    "    training_info = f\"Epochs: {CONFIG['num_train_epochs']}\"\n",
    "\n",
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- vi\n",
    "- en\n",
    "tags:\n",
    "- vision-language-model\n",
    "- vlm\n",
    "- ouro\n",
    "- looplm\n",
    "- fastvlm\n",
    "- vietnamese\n",
    "base_model: {CONFIG['llm_model']}\n",
    "datasets:\n",
    "- {CONFIG['dataset_name']}\n",
    "---\n",
    "\n",
    "# Belle-VLM-Ouro: Vietnamese Vision Language Model\n",
    "\n",
    "Built on **ByteDance Ouro-1.4B** (Looped Language Model).\n",
    "\n",
    "## Architecture\n",
    "- **LLM**: Ouro-1.4B (LoopLM, 4 recurrent steps)\n",
    "- **Vision**: FastViTHD (MobileCLIP)\n",
    "- **Projector**: MLP 3072 -> 2048\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{CONFIG['hf_repo']}\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Training\n",
    "- Dataset: {CONFIG['dataset_name']}\n",
    "- {training_info}\n",
    "- LoRA: r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(MERGED_DIR, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE AND UPLOAD CUSTOM CODE FILES\n",
    "# ============================================\n",
    "# These files are REQUIRED for trust_remote_code=True\n",
    "\n",
    "# 1. configuration_llava_ouro.py\n",
    "config_code = '''# Configuration for LLaVA Ouro model\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "class LlavaOuroConfig(PretrainedConfig):\n",
    "    \"\"\"Configuration class for LLaVA Ouro model (LoopLM architecture).\"\"\"\n",
    "    model_type = \"llava_ouro\"\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=49152, hidden_size=2048, intermediate_size=5632,\n",
    "        num_hidden_layers=24, num_attention_heads=16, num_key_value_heads=16,\n",
    "        hidden_act=\"silu\", max_position_embeddings=65536, initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6, use_cache=True, tie_word_embeddings=False,\n",
    "        rope_theta=1000000.0, rope_scaling=None, use_sliding_window=False,\n",
    "        sliding_window=4096, max_window_layers=28, layer_types=None, attention_dropout=0.0,\n",
    "        total_ut_steps=4, early_exit_threshold=1.0,\n",
    "        mm_vision_tower=None, mm_hidden_size=None, mm_projector_type=\"mlp2x_gelu\",\n",
    "        mm_vision_select_layer=-2, mm_vision_select_feature=\"patch\",\n",
    "        mm_patch_merge_type=\"flat\", mm_use_im_start_end=False,\n",
    "        mm_use_im_patch_token=False, image_aspect_ratio=\"pad\", **kwargs\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.use_sliding_window = use_sliding_window\n",
    "        self.sliding_window = sliding_window if use_sliding_window else None\n",
    "        self.max_window_layers = max_window_layers\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.total_ut_steps = total_ut_steps\n",
    "        self.early_exit_threshold = early_exit_threshold\n",
    "        self.layer_types = layer_types or [\"full_attention\"] * num_hidden_layers\n",
    "        self.mm_vision_tower = mm_vision_tower\n",
    "        self.mm_hidden_size = mm_hidden_size\n",
    "        self.mm_projector_type = mm_projector_type\n",
    "        self.mm_vision_select_layer = mm_vision_select_layer\n",
    "        self.mm_vision_select_feature = mm_vision_select_feature\n",
    "        self.mm_patch_merge_type = mm_patch_merge_type\n",
    "        self.mm_use_im_start_end = mm_use_im_start_end\n",
    "        self.mm_use_im_patch_token = mm_use_im_patch_token\n",
    "        self.image_aspect_ratio = image_aspect_ratio\n",
    "        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/configuration_llava_ouro.py\", \"w\") as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(\"Created configuration_llava_ouro.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. modeling_llava_ouro.py (simplified for inference)\n",
    "modeling_code = '''# LLaVA Ouro Model for HuggingFace\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from .configuration_llava_ouro import LlavaOuroConfig\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "\n",
    "def build_vision_projector(config):\n",
    "    projector_type = getattr(config, \"mm_projector_type\", \"mlp2x_gelu\")\n",
    "    mm_hidden_size = getattr(config, \"mm_hidden_size\", 3072)\n",
    "    hidden_size = config.hidden_size\n",
    "    if projector_type == \"mlp2x_gelu\":\n",
    "        return nn.Sequential(nn.Linear(mm_hidden_size, hidden_size), nn.GELU(), nn.Linear(hidden_size, hidden_size))\n",
    "    return nn.Linear(mm_hidden_size, hidden_size)\n",
    "\n",
    "class MobileCLIPVisionTower(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.is_loaded = False\n",
    "        self.image_processor = None\n",
    "        self.vision_tower = None\n",
    "        self.hidden_size = getattr(config, \"mm_hidden_size\", 3072)\n",
    "        try:\n",
    "            self.image_size = int(getattr(config, \"mm_vision_tower\", \"mobileclip_l_384\").split(\"_\")[-1])\n",
    "        except:\n",
    "            self.image_size = 384\n",
    "\n",
    "    def load_model(self, **kwargs):\n",
    "        if self.is_loaded:\n",
    "            return\n",
    "        try:\n",
    "            import timm\n",
    "            from transformers import CLIPImageProcessor\n",
    "            self.vision_tower = timm.create_model(\"fastvit_mci2.apple_mclip\", pretrained=True, num_classes=0)\n",
    "            self.vision_tower.eval()\n",
    "            self.image_processor = CLIPImageProcessor(size={\"shortest_edge\": self.image_size},\n",
    "                crop_size={\"height\": self.image_size, \"width\": self.image_size}, do_normalize=True,\n",
    "                image_mean=[0.48145466, 0.4578275, 0.40821073], image_std=[0.26862954, 0.26130258, 0.27577711])\n",
    "            self.is_loaded = True\n",
    "            print(\"MobileCLIP loaded!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load MobileCLIP: {e}\")\n",
    "            self.is_loaded = True\n",
    "\n",
    "    def forward(self, images):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        with torch.no_grad():\n",
    "            features = self.vision_tower.forward_features(images) if hasattr(self.vision_tower, \"forward_features\") else self.vision_tower(images)\n",
    "        if features.dim() == 4:\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.flatten(2).transpose(1, 2)\n",
    "        elif features.dim() == 2:\n",
    "            features = features.unsqueeze(1)\n",
    "        return features\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super().to(*args, **kwargs)\n",
    "        if self.vision_tower is not None:\n",
    "            self.vision_tower = self.vision_tower.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "class LlavaOuroForCausalLM(PreTrainedModel):\n",
    "    config_class = LlavaOuroConfig\n",
    "    _no_split_modules = [\"OuroDecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: LlavaOuroConfig):\n",
    "        super().__init__(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        # Base model will be loaded from pretrained\n",
    "        self.model = None\n",
    "        self.lm_head = None\n",
    "        # Vision modules\n",
    "        if hasattr(config, \"mm_vision_tower\") and config.mm_vision_tower:\n",
    "            self.vision_tower = MobileCLIPVisionTower(config)\n",
    "            self.mm_projector = build_vision_projector(config)\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        return getattr(self, \"vision_tower\", None)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        if config is None:\n",
    "            config = LlavaOuroConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        \n",
    "        # Load base Ouro model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"ByteDance/Ouro-1.4B\", trust_remote_code=True,\n",
    "            torch_dtype=kwargs.get(\"torch_dtype\", torch.float16),\n",
    "            device_map=kwargs.get(\"device_map\", \"auto\"),\n",
    "        )\n",
    "        \n",
    "        # Create LLaVA wrapper\n",
    "        model = cls(config)\n",
    "        model.model = base_model.model\n",
    "        model.lm_head = base_model.lm_head\n",
    "        \n",
    "        # Load mm_projector weights from safetensors\n",
    "        try:\n",
    "            from safetensors import safe_open\n",
    "            import os\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                sf_path = os.path.join(pretrained_model_name_or_path, \"model.safetensors\")\n",
    "            else:\n",
    "                from huggingface_hub import hf_hub_download\n",
    "                sf_path = hf_hub_download(pretrained_model_name_or_path, \"model.safetensors\")\n",
    "            \n",
    "            with safe_open(sf_path, framework=\"pt\") as f:\n",
    "                for key in f.keys():\n",
    "                    if \"mm_projector\" in key:\n",
    "                        clean_key = key.replace(\"model.mm_projector.\", \"\")\n",
    "                        model.mm_projector.state_dict()[clean_key].copy_(f.get_tensor(key))\n",
    "            print(\"Loaded mm_projector weights\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load mm_projector: {e}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, position_ids=None, past_key_values=None,\n",
    "                inputs_embeds=None, labels=None, images=None, image_sizes=None, **kwargs):\n",
    "        # Simple forward - for full implementation see modeling_llava_ouro.py\n",
    "        if inputs_embeds is None and input_ids is not None:\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "        \n",
    "        outputs = self.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask,\n",
    "                            position_ids=position_ids, past_key_values=past_key_values, **kwargs)\n",
    "        \n",
    "        hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs.last_hidden_state\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits[..., :-1, :].contiguous().view(-1, self.vocab_size),\n",
    "                           labels[..., 1:].contiguous().view(-1))\n",
    "        \n",
    "        return CausalLMOutputWithPast(loss=loss, logits=logits, past_key_values=getattr(outputs, \"past_key_values\", None))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs=None, images=None, **kwargs):\n",
    "        if images is not None and self.get_vision_tower() is not None:\n",
    "            vt = self.get_vision_tower()\n",
    "            if not vt.is_loaded:\n",
    "                vt.load_model()\n",
    "                vt = vt.to(device=images.device, dtype=images.dtype)\n",
    "            img_features = vt(images)\n",
    "            img_features = self.mm_projector(img_features)\n",
    "            inputs_embeds = self.model.embed_tokens(inputs)\n",
    "            inputs_embeds = torch.cat([img_features, inputs_embeds], dim=1)\n",
    "            kwargs[\"inputs_embeds\"] = inputs_embeds\n",
    "            kwargs[\"attention_mask\"] = torch.ones(inputs_embeds.shape[:2], device=inputs_embeds.device)\n",
    "            inputs = None\n",
    "        return super().generate(inputs, **kwargs)\n",
    "\n",
    "AutoConfig.register(\"llava_ouro\", LlavaOuroConfig)\n",
    "AutoModelForCausalLM.register(LlavaOuroConfig, LlavaOuroForCausalLM)\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/modeling_llava_ouro.py\", \"w\") as f:\n",
    "    f.write(modeling_code)\n",
    "\n",
    "print(\"Created modeling_llava_ouro.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UPLOAD TO HUGGINGFACE (with custom code)\n",
    "# ============================================\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "api = HfApi(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# Create repo\n",
    "create_repo(CONFIG[\"hf_repo\"], exist_ok=True, token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# 1. Upload model files\n",
    "print(f\"Uploading model to {CONFIG['hf_repo']}...\")\n",
    "api.upload_folder(\n",
    "    folder_path=MERGED_DIR,\n",
    "    repo_id=CONFIG[\"hf_repo\"],\n",
    "    commit_message=\"Upload Belle-VLM-Ouro model\",\n",
    ")\n",
    "print(\"Model uploaded!\")\n",
    "\n",
    "# 2. Upload custom code files (REQUIRED for trust_remote_code)\n",
    "print(\"\\nUploading custom code files...\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/tmp/configuration_llava_ouro.py\",\n",
    "    path_in_repo=\"configuration_llava_ouro.py\",\n",
    "    repo_id=CONFIG[\"hf_repo\"],\n",
    ")\n",
    "print(\"Uploaded configuration_llava_ouro.py\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/tmp/modeling_llava_ouro.py\",\n",
    "    path_in_repo=\"modeling_llava_ouro.py\",\n",
    "    repo_id=CONFIG[\"hf_repo\"],\n",
    ")\n",
    "print(\"Uploaded modeling_llava_ouro.py\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Model uploaded successfully!\")\n",
    "print(f\"https://huggingface.co/{CONFIG['hf_repo']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST INFERENCE\n",
    "# ============================================\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import timm\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "print(\"Loading model for inference...\")\n",
    "\n",
    "# Load from merged directory or HuggingFace\n",
    "MODEL_PATH = MERGED_DIR  # or CONFIG[\"hf_repo\"]\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load vision tower\n",
    "vision_tower = timm.create_model(\"fastvit_mci2.apple_mclip\", pretrained=True, num_classes=0)\n",
    "vision_tower.eval()\n",
    "vision_tower = vision_tower.to(model.device, dtype=torch.float16)\n",
    "\n",
    "# Image processor\n",
    "image_processor = CLIPImageProcessor(\n",
    "    size={\"shortest_edge\": 384},\n",
    "    crop_size={\"height\": 384, \"width\": 384},\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "    image_std=[0.26862954, 0.26130258, 0.27577711],\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def inference(image_path, question, max_new_tokens=512):\n",
    "    \"\"\"Run inference on a single image.\"\"\"\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
    "    \n",
    "    # Get image features\n",
    "    with torch.no_grad():\n",
    "        image_features = vision_tower.forward_features(image_tensor)\n",
    "        if image_features.dim() == 4:\n",
    "            B, C, H, W = image_features.shape\n",
    "            image_features = image_features.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Project to LLM space\n",
    "        image_features = model.mm_projector(image_features)\n",
    "    \n",
    "    # Tokenize question\n",
    "    prompt = f\"User: {question}\\nAssistant:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Get text embeddings\n",
    "    text_embeds = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "    \n",
    "    # Concatenate image + text\n",
    "    inputs_embeds = torch.cat([image_features, text_embeds], dim=1)\n",
    "    attention_mask = torch.ones(inputs_embeds.shape[:2], device=model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "print(\"Inference function ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
