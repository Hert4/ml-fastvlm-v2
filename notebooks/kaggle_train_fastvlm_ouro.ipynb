{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastVLM Training with Ouro-1.4B (LoopLM)\n",
    "\n",
    "Train FastVLM using **ByteDance Ouro-1.4B** - a Looped Language Model.\n",
    "\n",
    "- **LLM Backbone**: Ouro-1.4B (LoopLM architecture)\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP)\n",
    "- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified\n",
    "\n",
    "## Ouro Architecture\n",
    "- **Hidden Size**: 2048\n",
    "- **Layers**: 24 (x4 recurrent steps)\n",
    "- **Effective capacity**: ~4-12B model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# IMPORTANT: Ouro requires transformers < 4.56.0\n",
    "!pip install -q transformers==4.54.1\n",
    "!pip install -q torch>=2.1.0 torchvision>=0.16.0\n",
    "!pip install -q accelerate>=0.26.0 peft>=0.10.0\n",
    "!pip install -q bitsandbytes>=0.43.0\n",
    "!pip install -q datasets pillow einops timm>=0.9.0\n",
    "!pip install -q sentencepiece safetensors\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify transformers version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "assert transformers.__version__ < \"4.56.0\", \"Ouro requires transformers < 4.56.0!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - OURO-1.4B\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model - OURO\n",
    "    \"llm_model\": \"ByteDance/Ouro-1.4B\",\n",
    "    \"vision_tower\": \"mobileclip_l_384\",\n",
    "    \"mm_hidden_size\": 3072,       # MobileCLIP output\n",
    "    \"llm_hidden_size\": 2048,      # Ouro hidden size\n",
    "    \n",
    "    # Dataset\n",
    "    \"dataset_name\": \"5CD-AI/Viet-multimodal-open-r1-8k-verified\",\n",
    "    \"image_column\": \"image\",\n",
    "    \"question_column\": \"vi_problem\",\n",
    "    \"answer_column\": \"vi_solution\",\n",
    "    \n",
    "    # Training\n",
    "    \"output_dir\": \"./outputs/fastvlm-ouro-1.4b-vietnamese\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,  # Ouro is larger, need smaller batch\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"bf16\": True,\n",
    "    \"model_max_length\": 2048,\n",
    "    \n",
    "    # LoRA\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Save\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # HuggingFace\n",
    "    \"hf_repo\": \"beyoru/Belle-VLM-Ouro\",\n",
    "    \"hf_token\": os.environ.get(\"HF_TOKEN\", \"\"),\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration (Ouro-1.4B):\")\n",
    "for k, v in CONFIG.items():\n",
    "    if k != \"hf_token\":\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "    print(\"Logged in to HuggingFace!\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Ouro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "print(f\"Loading Ouro model: {CONFIG['llm_model']}\")\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=CONFIG[\"model_max_length\"],\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Load Ouro model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Total UT steps: {getattr(model.config, 'total_ut_steps', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vision modules to Ouro\n",
    "import torch.nn as nn\n",
    "\n",
    "# Build mm_projector: 3072 (MobileCLIP) -> 2048 (Ouro)\n",
    "mm_projector = nn.Sequential(\n",
    "    nn.Linear(CONFIG[\"mm_hidden_size\"], CONFIG[\"llm_hidden_size\"]),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(CONFIG[\"llm_hidden_size\"], CONFIG[\"llm_hidden_size\"]),\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "# Attach to model\n",
    "model.mm_projector = mm_projector\n",
    "\n",
    "print(f\"Added mm_projector: {CONFIG['mm_hidden_size']} -> {CONFIG['llm_hidden_size']}\")\n",
    "\n",
    "# Count parameters\n",
    "proj_params = sum(p.numel() for p in mm_projector.parameters())\n",
    "print(f\"mm_projector parameters: {proj_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# LoRA config for Ouro\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Vision Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "# Load MobileCLIP vision tower\n",
    "print(\"Loading MobileCLIP vision tower...\")\n",
    "vision_tower = timm.create_model(\n",
    "    \"fastvit_mci2.apple_mclip\",\n",
    "    pretrained=True,\n",
    "    num_classes=0,\n",
    ")\n",
    "vision_tower.eval()\n",
    "vision_tower = vision_tower.to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "# Image processor\n",
    "image_processor = CLIPImageProcessor(\n",
    "    size={\"shortest_edge\": 384},\n",
    "    crop_size={\"height\": 384, \"width\": 384},\n",
    "    do_center_crop=True,\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "    image_std=[0.26862954, 0.26130258, 0.27577711],\n",
    ")\n",
    "\n",
    "# Test\n",
    "dummy_img = torch.randn(1, 3, 384, 384).to(model.device, dtype=torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    features = vision_tower.forward_features(dummy_img)\n",
    "print(f\"Vision tower output: {features.shape}\")\n",
    "\n",
    "# Attach to model\n",
    "model.vision_tower = vision_tower\n",
    "model.image_processor = image_processor\n",
    "\n",
    "print(\"Vision tower ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Columns: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLaVA format data\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, \"images\")\n",
    "os.makedirs(IMAGE_FOLDER, exist_ok=True)\n",
    "\n",
    "llava_data = []\n",
    "\n",
    "for idx, sample in enumerate(tqdm(dataset, desc=\"Converting\")):\n",
    "    # Save image\n",
    "    image_filename = f\"{idx:06d}.jpg\"\n",
    "    image_path = os.path.join(IMAGE_FOLDER, image_filename)\n",
    "    \n",
    "    img = sample['image']\n",
    "    if isinstance(img, Image.Image):\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img.save(image_path, 'JPEG', quality=95)\n",
    "    \n",
    "    # Conversation\n",
    "    question = sample['vi_problem'].strip()\n",
    "    answer = sample['vi_solution'].strip()\n",
    "    if len(answer) > 4096:\n",
    "        answer = answer[:4096] + \"...\"\n",
    "    \n",
    "    llava_data.append({\n",
    "        \"id\": str(idx),\n",
    "        \"image\": image_filename,\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": f\"<image>\\n{question}\"},\n",
    "            {\"from\": \"gpt\", \"value\": answer}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Save\n",
    "json_path = os.path.join(DATA_DIR, \"train_data.json\")\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(llava_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Dataset converted: {len(llava_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "\n",
    "class VLMDataset(Dataset):\n",
    "    \"\"\"Simple VLM dataset for training.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, image_folder, tokenizer, image_processor, max_length=2048):\n",
    "        self.data = data\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711]\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_folder, item['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        # Build conversation\n",
    "        conv = item['conversations']\n",
    "        question = conv[0]['value'].replace('<image>', '').strip()\n",
    "        answer = conv[1]['value']\n",
    "        \n",
    "        # Format prompt (Ouro uses standard format)\n",
    "        prompt = f\"User: <image>\\n{question}\\nAssistant: {answer}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Labels (mask prompt, only train on answer)\n",
    "        labels = input_ids.clone()\n",
    "        # Find where answer starts\n",
    "        answer_start = prompt.find('Assistant:') + len('Assistant:')\n",
    "        answer_tokens = self.tokenizer(prompt[:answer_start], return_tensors='pt')['input_ids'].shape[1]\n",
    "        labels[:answer_tokens] = -100  # Ignore prompt tokens\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'images': image_tensor,\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = VLMDataset(\n",
    "    llava_data, \n",
    "    IMAGE_FOLDER, \n",
    "    tokenizer, \n",
    "    image_processor,\n",
    "    max_length=CONFIG[\"model_max_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    logging_steps=10,\n",
    "    save_strategy=CONFIG[\"save_strategy\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch]),\n",
    "        'images': torch.stack([x['images'] for x in batch]),\n",
    "    }\n",
    "\n",
    "print(\"Training arguments ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for multimodal\n",
    "class VLMTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        images = inputs.pop('images').to(model.device, dtype=torch.bfloat16)\n",
    "        input_ids = inputs['input_ids']\n",
    "        labels = inputs['labels']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Encode images\n",
    "        with torch.no_grad():\n",
    "            image_features = model.vision_tower.forward_features(images)\n",
    "            if image_features.dim() == 4:\n",
    "                B, C, H, W = image_features.shape\n",
    "                image_features = image_features.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Project\n",
    "        image_features = model.mm_projector(image_features)\n",
    "        \n",
    "        # Get text embeddings\n",
    "        text_embeds = model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # Concatenate image + text\n",
    "        inputs_embeds = torch.cat([image_features, text_embeds], dim=1)\n",
    "        \n",
    "        # Adjust attention mask\n",
    "        image_mask = torch.ones(\n",
    "            images.size(0), image_features.size(1),\n",
    "            device=attention_mask.device, dtype=attention_mask.dtype\n",
    "        )\n",
    "        attention_mask = torch.cat([image_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # Adjust labels\n",
    "        image_labels = torch.full(\n",
    "            (images.size(0), image_features.size(1)),\n",
    "            -100,\n",
    "            device=labels.device, dtype=labels.dtype\n",
    "        )\n",
    "        labels = torch.cat([image_labels, labels], dim=1)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return (outputs.loss, outputs) if return_outputs else outputs.loss\n",
    "\n",
    "# Create trainer\n",
    "trainer = VLMTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(f\"Starting training for {CONFIG['num_train_epochs']} epochs...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(CONFIG[\"output_dir\"])\n",
    "\n",
    "# Save mm_projector separately\n",
    "mm_projector_path = os.path.join(CONFIG[\"output_dir\"], \"mm_projector.bin\")\n",
    "torch.save(model.mm_projector.state_dict(), mm_projector_path)\n",
    "print(f\"Saved mm_projector to {mm_projector_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "OUTPUT_DIR = CONFIG[\"output_dir\"]\n",
    "MERGED_DIR = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading base model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Load mm_projector\n",
    "mm_projector_weights = torch.load(mm_projector_path, map_location=\"cpu\")\n",
    "\n",
    "# Get state dict and add mm_projector\n",
    "state_dict = merged_model.state_dict()\n",
    "for k, v in mm_projector_weights.items():\n",
    "    state_dict[f\"model.mm_projector.{k}\"] = v.to(torch.float16)\n",
    "    print(f\"Added: model.mm_projector.{k}\")\n",
    "\n",
    "# Save\n",
    "merged_model.save_pretrained(MERGED_DIR, state_dict=state_dict, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(f\"\\nModel saved to: {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for LLaVA-Ouro\n",
    "import json\n",
    "\n",
    "config_data = merged_model.config.to_dict()\n",
    "config_data[\"model_type\"] = \"llava_ouro\"\n",
    "config_data[\"architectures\"] = [\"LlavaOuroForCausalLM\"]\n",
    "config_data[\"mm_vision_tower\"] = CONFIG[\"vision_tower\"]\n",
    "config_data[\"mm_hidden_size\"] = CONFIG[\"mm_hidden_size\"]\n",
    "config_data[\"mm_projector_type\"] = \"mlp2x_gelu\"\n",
    "config_data[\"auto_map\"] = {\n",
    "    \"AutoConfig\": \"configuration_llava_ouro.LlavaOuroConfig\",\n",
    "    \"AutoModelForCausalLM\": \"modeling_llava_ouro.LlavaOuroForCausalLM\"\n",
    "}\n",
    "\n",
    "config_path = os.path.join(MERGED_DIR, \"config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_data, f, indent=2)\n",
    "\n",
    "print(\"Config saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "from safetensors import safe_open\n",
    "\n",
    "safetensor_path = os.path.join(MERGED_DIR, \"model.safetensors\")\n",
    "print(f\"Model size: {os.path.getsize(safetensor_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "with safe_open(safetensor_path, framework=\"pt\") as f:\n",
    "    mm_keys = [k for k in f.keys() if 'mm_projector' in k]\n",
    "    if mm_keys:\n",
    "        print(\"\\nmm_projector found:\")\n",
    "        for k in mm_keys:\n",
    "            print(f\"  {k}: {f.get_tensor(k).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card\n",
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- vi\n",
    "- en\n",
    "tags:\n",
    "- vision-language-model\n",
    "- vlm\n",
    "- ouro\n",
    "- looplm\n",
    "- fastvlm\n",
    "- vietnamese\n",
    "base_model: {CONFIG['llm_model']}\n",
    "datasets:\n",
    "- {CONFIG['dataset_name']}\n",
    "---\n",
    "\n",
    "# Belle-VLM-Ouro: Vietnamese Vision Language Model\n",
    "\n",
    "Built on **ByteDance Ouro-1.4B** (Looped Language Model).\n",
    "\n",
    "## Architecture\n",
    "- **LLM**: Ouro-1.4B (LoopLM, 4 recurrent steps)\n",
    "- **Vision**: FastViTHD (MobileCLIP)\n",
    "- **Projector**: MLP 3072 -> 2048\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{CONFIG['hf_repo']}\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Training\n",
    "- Dataset: {CONFIG['dataset_name']}\n",
    "- Epochs: {CONFIG['num_train_epochs']}\n",
    "- LoRA: r={CONFIG['lora_r']}, alpha={CONFIG['lora_alpha']}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(MERGED_DIR, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "api = HfApi(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "create_repo(CONFIG[\"hf_repo\"], exist_ok=True, token=CONFIG[\"hf_token\"])\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=MERGED_DIR,\n",
    "    repo_id=CONFIG[\"hf_repo\"],\n",
    "    commit_message=\"Upload Belle-VLM-Ouro\",\n",
    ")\n",
    "\n",
    "print(f\"\\nUploaded to: https://huggingface.co/{CONFIG['hf_repo']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Model trained with **Ouro-1.4B** and uploaded to HuggingFace.\n",
    "\n",
    "Ouro's LoopLM architecture provides:\n",
    "- 2-3x parameter efficiency\n",
    "- 4-12B model performance with only 1.4B params\n",
    "- Adaptive computation via early exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
