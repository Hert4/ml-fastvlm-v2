{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastVLM Training with Qwen3-0.6B on Kaggle\n",
    "\n",
    "This notebook trains FastVLM (Vision Language Model) using:\n",
    "- **LLM Backbone**: Qwen3-0.6B\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP)\n",
    "- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified\n",
    "- **Output**: Push to HuggingFace: beyoru/Belle-VLM\n",
    "\n",
    "## Requirements\n",
    "- Kaggle GPU (P100/T4/A100)\n",
    "- ~16GB VRAM for full training, or use QLoRA for less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.51.0,<5.0.0\n",
    "!pip install -q torch>=2.1.0 torchvision>=0.16.0\n",
    "!pip install -q accelerate>=0.26.0 peft>=0.10.0\n",
    "!pip install -q bitsandbytes>=0.43.0\n",
    "!pip install -q datasets pillow einops timm>=0.9.0\n",
    "!pip install -q deepspeed sentencepiece\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the FastVLM repository\n",
    "!git clone https://github.com/Hert4/ml-fastvlm-v2.git\n",
    "%cd ml-fastvlm-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Qwen3 support\n",
    "!python test_qwen3_load.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport torch\nfrom pathlib import Path\n\n# Configuration\nCONFIG = {\n    # Model\n    \"llm_model\": \"Qwen/Qwen3-0.6B\",\n    # Vision tower format: mobileclip_l_<image_size>\n    # Available: mobileclip_l_256, mobileclip_l_384, mobileclip_l_512\n    # The config uses fastvithd backbone with embed_dim=3072\n    \"vision_tower\": \"mobileclip_l_384\",  # FastViTHD with 384x384 input\n    \n    # Dataset\n    \"dataset_name\": \"5CD-AI/Viet-multimodal-open-r1-8k-verified\",\n    \"image_column\": \"image\",\n    \"question_column\": \"vi_problem\",\n    \"answer_column\": \"vi_solution\",\n    \n    # Training\n    \"output_dir\": \"./outputs/fastvlm-qwen3-0.6b-vietnamese\",\n    \"num_train_epochs\": 3,\n    \"per_device_train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 8,\n    \"learning_rate\": 2e-5,\n    \"warmup_ratio\": 0.03,\n    \"lr_scheduler_type\": \"cosine\",\n    \"bf16\": True,\n    \"model_max_length\": 2048,\n    \n    # LoRA (for memory efficiency)\n    \"use_lora\": True,\n    \"lora_r\": 64,\n    \"lora_alpha\": 128,\n    \"lora_dropout\": 0.05,\n    \n    # HuggingFace - Set your token in Kaggle Secrets as HF_TOKEN\n    \"hf_repo\": \"beyoru/Belle-VLM\",\n    \"hf_token\": os.environ.get(\"HF_TOKEN\", \"\"),  # Get from environment/Kaggle secrets\n}\n\n# Create output directory\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    if k != \"hf_token\":\n        print(f\"  {k}: {v}\")\n\nif not CONFIG[\"hf_token\"]:\n    print(\"\\n⚠️ WARNING: HF_TOKEN not set! Set it in Kaggle Secrets to push model to HuggingFace.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace\nfrom huggingface_hub import login\n\nif CONFIG[\"hf_token\"]:\n    login(token=CONFIG[\"hf_token\"])\n    print(\"✅ Logged in to HuggingFace!\")\nelse:\n    print(\"⚠️ HF_TOKEN not set. To push to HuggingFace, set your token:\")\n    print(\"   1. Go to Kaggle notebook settings\")\n    print(\"   2. Add Secret: Name='HF_TOKEN', Value='your_hf_token'\")\n    print(\"   3. Enable 'Add-ons' > 'Secrets' in your notebook\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Columns: {dataset.column_names}\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"  vi_problem: {sample['vi_problem'][:200]}...\")\n",
    "print(f\"  vi_solution: {sample['vi_solution'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample image\n",
    "from IPython.display import display\n",
    "\n",
    "sample_image = sample['image']\n",
    "if isinstance(sample_image, Image.Image):\n",
    "    display(sample_image.resize((400, 400)))\n",
    "else:\n",
    "    print(f\"Image type: {type(sample_image)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_dataset_to_llava_format(dataset, output_dir, image_folder):\n",
    "    \"\"\"\n",
    "    Convert dataset to LLaVA training format.\n",
    "    \n",
    "    LLaVA format:\n",
    "    {\n",
    "        \"id\": \"unique_id\",\n",
    "        \"image\": \"image_filename.jpg\",\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"<image>\\nQuestion\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Answer\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    llava_data = []\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=\"Converting dataset\")):\n",
    "        # Save image\n",
    "        image_filename = f\"{idx:06d}.jpg\"\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        \n",
    "        img = sample['image']\n",
    "        if isinstance(img, Image.Image):\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img.save(image_path, 'JPEG', quality=95)\n",
    "        \n",
    "        # Create conversation\n",
    "        question = sample['vi_problem'].strip()\n",
    "        answer = sample['vi_solution'].strip()\n",
    "        \n",
    "        # Truncate very long solutions to avoid memory issues\n",
    "        max_answer_len = 4096\n",
    "        if len(answer) > max_answer_len:\n",
    "            answer = answer[:max_answer_len] + \"...\"\n",
    "        \n",
    "        llava_sample = {\n",
    "            \"id\": str(sample.get('id', idx)),\n",
    "            \"image\": image_filename,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<image>\\n{question}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        llava_data.append(llava_sample)\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = os.path.join(output_dir, \"train_data.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(llava_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nDataset converted:\")\n",
    "    print(f\"  JSON file: {json_path}\")\n",
    "    print(f\"  Images: {image_folder}\")\n",
    "    print(f\"  Total samples: {len(llava_data)}\")\n",
    "    \n",
    "    return json_path, image_folder\n",
    "\n",
    "# Convert dataset\n",
    "DATA_DIR = \"./data\"\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, \"images\")\n",
    "\n",
    "json_path, image_folder = convert_dataset_to_llava_format(\n",
    "    dataset, \n",
    "    DATA_DIR, \n",
    "    IMAGE_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create training script arguments\nimport sys\nsys.path.insert(0, '.')\n\ntraining_args = {\n    # Model\n    \"model_name_or_path\": CONFIG[\"llm_model\"],\n    \"version\": \"qwen_3\",\n    \"vision_tower\": CONFIG[\"vision_tower\"],  # mobileclip_l_384\n    \"mm_projector_type\": \"mlp2x_gelu\",\n    \n    # Data\n    \"data_path\": json_path,\n    \"image_folder\": image_folder,\n    \"image_aspect_ratio\": \"pad\",\n    \n    # Training\n    \"output_dir\": CONFIG[\"output_dir\"],\n    \"num_train_epochs\": CONFIG[\"num_train_epochs\"],\n    \"per_device_train_batch_size\": CONFIG[\"per_device_train_batch_size\"],\n    \"gradient_accumulation_steps\": CONFIG[\"gradient_accumulation_steps\"],\n    \"learning_rate\": CONFIG[\"learning_rate\"],\n    \"warmup_ratio\": CONFIG[\"warmup_ratio\"],\n    \"lr_scheduler_type\": CONFIG[\"lr_scheduler_type\"],\n    \"model_max_length\": CONFIG[\"model_max_length\"],\n    \n    # Precision\n    \"bf16\": CONFIG[\"bf16\"],\n    \n    # LoRA\n    \"lora_enable\": CONFIG[\"use_lora\"],\n    \"lora_r\": CONFIG[\"lora_r\"],\n    \"lora_alpha\": CONFIG[\"lora_alpha\"],\n    \"lora_dropout\": CONFIG[\"lora_dropout\"],\n    \n    # Other\n    \"gradient_checkpointing\": True,\n    \"dataloader_num_workers\": 4,\n    \"logging_steps\": 1,\n    \"save_steps\": 500,\n    \"save_total_limit\": 2,\n    \"report_to\": \"none\",\n}\n\nprint(\"Training arguments prepared!\")\nprint(f\"Vision tower: {CONFIG['vision_tower']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Direct Python training (recommended for Kaggle)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    model_max_length=CONFIG[\"model_max_length\"],\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {CONFIG['llm_model']}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with QLoRA (4-bit quantization for memory efficiency)\n",
    "from llava.model.language_model.llava_qwen import LlavaQwen3ForCausalLM, LlavaQwen3Config\n",
    "\n",
    "# Quantization config for 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = LlavaQwen3ForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Model type: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# Setup LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run training using the train_qwen.py script\nimport subprocess\nimport torch\nimport os\n\n# Force single GPU to avoid DataParallel issues with variable-length sequences\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Check GPU capability for TF32 support (requires Ampere or newer)\ndef supports_tf32():\n    if torch.cuda.is_available():\n        capability = torch.cuda.get_device_capability()\n        # TF32 requires compute capability >= 8.0 (Ampere)\n        return capability[0] >= 8\n    return False\n\nuse_tf32 = supports_tf32()\nprint(f\"GPU supports TF32: {use_tf32}\")\n\n# Build command\ncmd = [\n    \"python\", \"-m\", \"llava.train.train_qwen\",\n    f\"--model_name_or_path={CONFIG['llm_model']}\",\n    \"--version=qwen_3\",\n    f\"--data_path={json_path}\",\n    f\"--image_folder={image_folder}\",\n    f\"--vision_tower={CONFIG['vision_tower']}\",  # mobileclip_l_384\n    \"--mm_projector_type=mlp2x_gelu\",\n    \"--mm_vision_select_layer=-2\",\n    \"--mm_use_im_start_end=False\",\n    \"--mm_use_im_patch_token=False\",\n    \"--image_aspect_ratio=pad\",\n    f\"--output_dir={CONFIG['output_dir']}\",\n    f\"--num_train_epochs={CONFIG['num_train_epochs']}\",\n    f\"--per_device_train_batch_size={CONFIG['per_device_train_batch_size']}\",\n    f\"--gradient_accumulation_steps={CONFIG['gradient_accumulation_steps']}\",\n    f\"--learning_rate={CONFIG['learning_rate']}\",\n    \"--weight_decay=0.0\",\n    f\"--warmup_ratio={CONFIG['warmup_ratio']}\",\n    f\"--lr_scheduler_type={CONFIG['lr_scheduler_type']}\",\n    \"--logging_steps=1\",\n    \"--save_steps=500\",\n    \"--save_total_limit=2\",\n    f\"--model_max_length={CONFIG['model_max_length']}\",\n    \"--gradient_checkpointing=True\",\n    \"--dataloader_num_workers=4\",\n    \"--lazy_preprocess=True\",\n    \"--bf16=True\",\n    \"--report_to=none\",\n    # LoRA\n    \"--lora_enable=True\",\n    f\"--lora_r={CONFIG['lora_r']}\",\n    f\"--lora_alpha={CONFIG['lora_alpha']}\",\n    f\"--lora_dropout={CONFIG['lora_dropout']}\",\n]\n\n# Only add TF32 if GPU supports it (Ampere A100+)\nif use_tf32:\n    cmd.append(\"--tf32=True\")\nelse:\n    cmd.append(\"--tf32=False\")\n\nprint(\"Starting training...\")\nprint(\"Using single GPU (CUDA_VISIBLE_DEVICES=0) to avoid DataParallel issues\")\nprint(\"Command:\", \" \".join(cmd[:5]), \"...\")\n\n# Run training with single GPU environment\nenv = os.environ.copy()\nenv[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nprocess = subprocess.Popen(\n    cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    universal_newlines=True,\n    bufsize=1,\n    env=env,\n)\n\n# Stream output\nfor line in process.stdout:\n    print(line, end='')\n\nprocess.wait()\nprint(f\"\\nTraining completed with return code: {process.returncode}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merge LoRA and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL = CONFIG[\"llm_model\"]\n",
    "LORA_PATH = CONFIG[\"output_dir\"]\n",
    "MERGED_PATH = os.path.join(CONFIG[\"output_dir\"], \"merged\")\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"LoRA path: {LORA_PATH}\")\n",
    "print(f\"Merged output: {MERGED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "print(\"Base model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge LoRA\n",
    "print(\"Loading LoRA weights...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "\n",
    "print(\"Merging LoRA weights...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"LoRA merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model locally\n",
    "print(f\"Saving merged model to {MERGED_PATH}...\")\n",
    "os.makedirs(MERGED_PATH, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(MERGED_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_PATH)\n",
    "\n",
    "# Also copy vision tower and projector weights if they exist\n",
    "import shutil\n",
    "for file in [\"mm_projector.bin\", \"config.json\"]:\n",
    "    src = os.path.join(LORA_PATH, file)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, os.path.join(MERGED_PATH, file))\n",
    "        print(f\"Copied: {file}\")\n",
    "\n",
    "print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Initialize API\n",
    "api = HfApi(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# Create repo if not exists\n",
    "try:\n",
    "    create_repo(\n",
    "        repo_id=CONFIG[\"hf_repo\"],\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True,\n",
    "        token=CONFIG[\"hf_token\"]\n",
    "    )\n",
    "    print(f\"Repository ready: {CONFIG['hf_repo']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repo exists or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card\n",
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- vi\n",
    "- en\n",
    "tags:\n",
    "- vision-language-model\n",
    "- vlm\n",
    "- qwen3\n",
    "- fastvlm\n",
    "- vietnamese\n",
    "base_model: {CONFIG['llm_model']}\n",
    "datasets:\n",
    "- {CONFIG['dataset_name']}\n",
    "---\n",
    "\n",
    "# Belle-VLM: Vietnamese Vision Language Model\n",
    "\n",
    "## Model Description\n",
    "\n",
    "Belle-VLM is a Vision Language Model trained for Vietnamese multimodal reasoning tasks.\n",
    "\n",
    "### Architecture\n",
    "- **LLM Backbone**: Qwen3-0.6B\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP-S2)\n",
    "- **Projector**: MLP 2-layer\n",
    "\n",
    "### Training\n",
    "- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified (7,030 samples)\n",
    "- **Method**: LoRA fine-tuning with QLoRA (4-bit)\n",
    "- **Epochs**: {CONFIG['num_train_epochs']}\n",
    "- **Learning Rate**: {CONFIG['learning_rate']}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{CONFIG['hf_repo']}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{CONFIG['hf_repo']}\")\n",
    "\n",
    "# For VLM inference, use with FastVLM pipeline\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Base Model | {CONFIG['llm_model']} |\n",
    "| LoRA Rank | {CONFIG['lora_r']} |\n",
    "| LoRA Alpha | {CONFIG['lora_alpha']} |\n",
    "| Batch Size | {CONFIG['per_device_train_batch_size']} x {CONFIG['gradient_accumulation_steps']} |\n",
    "| Max Length | {CONFIG['model_max_length']} |\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- FastVLM architecture from Apple\n",
    "- Qwen3 from Alibaba\n",
    "- Dataset from 5CD-AI\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "readme_path = os.path.join(MERGED_PATH, \"README.md\")\n",
    "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HuggingFace\n",
    "print(f\"Uploading to {CONFIG['hf_repo']}...\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=MERGED_PATH,\n",
    "    repo_id=CONFIG[\"hf_repo\"],\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload Belle-VLM trained on Vietnamese multimodal dataset\",\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Model uploaded successfully!\")\n",
    "print(f\"View at: https://huggingface.co/{CONFIG['hf_repo']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_path = MERGED_PATH\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, \"fastvlm-qwen3\", device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample image\n",
    "test_image = dataset[0]['image']\n",
    "test_question = dataset[0]['vi_problem']\n",
    "\n",
    "# Prepare input\n",
    "if test_image.mode != 'RGB':\n",
    "    test_image = test_image.convert('RGB')\n",
    "\n",
    "image_tensor = process_images([test_image], image_processor, model.config)[0]\n",
    "\n",
    "# Build prompt\n",
    "conv = conv_templates[\"qwen_3\"].copy()\n",
    "conv.append_message(conv.roles[0], f\"{DEFAULT_IMAGE_TOKEN}\\n{test_question}\")\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "# Generate\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor.unsqueeze(0).half().cuda(),\n",
    "        image_sizes=[test_image.size],\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "print(\"Question:\", test_question[:200], \"...\")\n",
    "print(\"\\nModel Response:\", output[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "The model has been:\n",
    "1. Trained on Vietnamese multimodal dataset\n",
    "2. Merged LoRA weights\n",
    "3. Pushed to HuggingFace\n",
    "\n",
    "View your model at: https://huggingface.co/beyoru/Belle-VLM"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}