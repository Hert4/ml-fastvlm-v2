{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastVLM Training with Qwen3-0.6B on Kaggle\n",
    "\n",
    "This notebook trains FastVLM (Vision Language Model) using:\n",
    "- **LLM Backbone**: Qwen3-0.6B\n",
    "- **Vision Encoder**: FastViTHD (MobileCLIP)\n",
    "- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified\n",
    "- **Output**: Push to HuggingFace: beyoru/Belle-VLM\n",
    "\n",
    "## Requirements\n",
    "- Kaggle GPU (P100/T4/A100)\n",
    "- ~16GB VRAM for full training, or use QLoRA for less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.51.0,<5.0.0\n",
    "!pip install -q torch>=2.1.0 torchvision>=0.16.0\n",
    "!pip install -q accelerate>=0.26.0 peft>=0.10.0\n",
    "!pip install -q bitsandbytes>=0.43.0\n",
    "!pip install -q datasets pillow einops timm>=0.9.0\n",
    "!pip install -q deepspeed sentencepiece\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the FastVLM repository\n",
    "!git clone https://github.com/Hert4/ml-fastvlm-v2.git\n",
    "%cd ml-fastvlm-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Qwen3 support\n",
    "!python test_qwen3_load.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport torch\nfrom pathlib import Path\n\n# Configuration\nCONFIG = {\n    # Model\n    \"llm_model\": \"Qwen/Qwen3-0.6B\",\n    \"vision_tower\": \"mobileclip_l_384\",\n    \n    # Dataset\n    \"dataset_name\": \"5CD-AI/Viet-multimodal-open-r1-8k-verified\",\n    \"image_column\": \"image\",\n    \"question_column\": \"vi_problem\",\n    \"answer_column\": \"vi_solution\",\n    \n    # Training - USE STEPS INSTEAD OF EPOCHS\n    \"output_dir\": \"./outputs/fastvlm-qwen3-0.6b-vietnamese\",\n    \"max_steps\": 2000,  # <-- CHANGE THIS (instead of num_train_epochs)\n    \"per_device_train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 8,\n    \"learning_rate\": 2e-5,\n    \"warmup_steps\": 100,  # <-- Changed from warmup_ratio\n    \"lr_scheduler_type\": \"cosine\",\n    \"bf16\": True,\n    \"model_max_length\": 2048,\n    \n    # LoRA\n    \"use_lora\": True,\n    \"lora_r\": 64,\n    \"lora_alpha\": 128,\n    \"lora_dropout\": 0.05,\n    \n    # Save checkpoints\n    \"save_steps\": 500,\n    \"save_total_limit\": 2,\n    \n    # HuggingFace\n    \"hf_repo\": \"beyoru/Belle-VLM\",\n    \"hf_token\": os.environ.get(\"HF_TOKEN\", \"\"),\n}\n\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    if k != \"hf_token\":\n        print(f\"  {k}: {v}\")\n\n# Estimate training time\neffective_batch = CONFIG[\"per_device_train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\nprint(f\"\\nðŸ“Š Training info:\")\nprint(f\"  Effective batch size: {effective_batch}\")\nprint(f\"  Total steps: {CONFIG['max_steps']}\")\nprint(f\"  Checkpoints at: {list(range(CONFIG['save_steps'], CONFIG['max_steps']+1, CONFIG['save_steps']))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace\nfrom huggingface_hub import login\n\nif CONFIG[\"hf_token\"]:\n    login(token=CONFIG[\"hf_token\"])\n    print(\"âœ… Logged in to HuggingFace!\")\nelse:\n    print(\"âš ï¸ HF_TOKEN not set. To push to HuggingFace, set your token:\")\n    print(\"   1. Go to Kaggle notebook settings\")\n    print(\"   2. Add Secret: Name='HF_TOKEN', Value='your_hf_token'\")\n    print(\"   3. Enable 'Add-ons' > 'Secrets' in your notebook\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "dataset = load_dataset(CONFIG[\"dataset_name\"], split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Columns: {dataset.column_names}\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"  vi_problem: {sample['vi_problem'][:200]}...\")\n",
    "print(f\"  vi_solution: {sample['vi_solution'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample image\n",
    "from IPython.display import display\n",
    "\n",
    "sample_image = sample['image']\n",
    "if isinstance(sample_image, Image.Image):\n",
    "    display(sample_image.resize((400, 400)))\n",
    "else:\n",
    "    print(f\"Image type: {type(sample_image)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_dataset_to_llava_format(dataset, output_dir, image_folder):\n",
    "    \"\"\"\n",
    "    Convert dataset to LLaVA training format.\n",
    "    \n",
    "    LLaVA format:\n",
    "    {\n",
    "        \"id\": \"unique_id\",\n",
    "        \"image\": \"image_filename.jpg\",\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"<image>\\nQuestion\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Answer\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    llava_data = []\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=\"Converting dataset\")):\n",
    "        # Save image\n",
    "        image_filename = f\"{idx:06d}.jpg\"\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        \n",
    "        img = sample['image']\n",
    "        if isinstance(img, Image.Image):\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img.save(image_path, 'JPEG', quality=95)\n",
    "        \n",
    "        # Create conversation\n",
    "        question = sample['vi_problem'].strip()\n",
    "        answer = sample['vi_solution'].strip()\n",
    "        \n",
    "        # Truncate very long solutions to avoid memory issues\n",
    "        max_answer_len = 4096\n",
    "        if len(answer) > max_answer_len:\n",
    "            answer = answer[:max_answer_len] + \"...\"\n",
    "        \n",
    "        llava_sample = {\n",
    "            \"id\": str(sample.get('id', idx)),\n",
    "            \"image\": image_filename,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<image>\\n{question}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        llava_data.append(llava_sample)\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = os.path.join(output_dir, \"train_data.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(llava_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nDataset converted:\")\n",
    "    print(f\"  JSON file: {json_path}\")\n",
    "    print(f\"  Images: {image_folder}\")\n",
    "    print(f\"  Total samples: {len(llava_data)}\")\n",
    "    \n",
    "    return json_path, image_folder\n",
    "\n",
    "# Convert dataset\n",
    "DATA_DIR = \"./data\"\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, \"images\")\n",
    "\n",
    "json_path, image_folder = convert_dataset_to_llava_format(\n",
    "    dataset, \n",
    "    DATA_DIR, \n",
    "    IMAGE_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training arguments (for reference)\nimport sys\nsys.path.insert(0, '.')\n\ntraining_args = {\n    \"model_name_or_path\": CONFIG[\"llm_model\"],\n    \"version\": \"qwen_3\",\n    \"vision_tower\": CONFIG[\"vision_tower\"],\n    \"mm_projector_type\": \"mlp2x_gelu\",\n    \"data_path\": json_path,\n    \"image_folder\": image_folder,\n    \"output_dir\": CONFIG[\"output_dir\"],\n    \n    # STEPS CONFIG\n    \"max_steps\": CONFIG[\"max_steps\"],\n    \"warmup_steps\": CONFIG[\"warmup_steps\"],\n    \"save_steps\": CONFIG[\"save_steps\"],\n    \n    \"per_device_train_batch_size\": CONFIG[\"per_device_train_batch_size\"],\n    \"gradient_accumulation_steps\": CONFIG[\"gradient_accumulation_steps\"],\n    \"learning_rate\": CONFIG[\"learning_rate\"],\n    \"lr_scheduler_type\": CONFIG[\"lr_scheduler_type\"],\n    \"bf16\": CONFIG[\"bf16\"],\n    \"lora_enable\": CONFIG[\"use_lora\"],\n    \"lora_r\": CONFIG[\"lora_r\"],\n    \"lora_alpha\": CONFIG[\"lora_alpha\"],\n}\n\nprint(\"Training config:\")\nprint(f\"  Max steps: {CONFIG['max_steps']}\")\nprint(f\"  Warmup steps: {CONFIG['warmup_steps']}\")\nprint(f\"  Save every: {CONFIG['save_steps']} steps\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Direct Python training (recommended for Kaggle)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    model_max_length=CONFIG[\"model_max_length\"],\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {CONFIG['llm_model']}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with QLoRA (4-bit quantization for memory efficiency)\n",
    "from llava.model.language_model.llava_qwen import LlavaQwen3ForCausalLM, LlavaQwen3Config\n",
    "\n",
    "# Quantization config for 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = LlavaQwen3ForCausalLM.from_pretrained(\n",
    "    CONFIG[\"llm_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Model type: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# Setup LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run training using the train_qwen.py script\nimport subprocess\nimport torch\nimport os\n\n# Force single GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Check TF32 support\ndef supports_tf32():\n    if torch.cuda.is_available():\n        return torch.cuda.get_device_capability()[0] >= 8\n    return False\n\nuse_tf32 = supports_tf32()\nprint(f\"GPU supports TF32: {use_tf32}\")\n\n# Build command - USING MAX_STEPS\ncmd = [\n    \"python\", \"-m\", \"llava.train.train_qwen\",\n    f\"--model_name_or_path={CONFIG['llm_model']}\",\n    \"--version=qwen_3\",\n    f\"--data_path={json_path}\",\n    f\"--image_folder={image_folder}\",\n    f\"--vision_tower={CONFIG['vision_tower']}\",\n    \"--mm_projector_type=mlp2x_gelu\",\n    \"--mm_vision_select_layer=-2\",\n    \"--mm_use_im_start_end=False\",\n    \"--mm_use_im_patch_token=False\",\n    \"--image_aspect_ratio=pad\",\n    f\"--output_dir={CONFIG['output_dir']}\",\n    # USE MAX_STEPS INSTEAD OF EPOCHS\n    f\"--max_steps={CONFIG['max_steps']}\",\n    f\"--per_device_train_batch_size={CONFIG['per_device_train_batch_size']}\",\n    f\"--gradient_accumulation_steps={CONFIG['gradient_accumulation_steps']}\",\n    f\"--learning_rate={CONFIG['learning_rate']}\",\n    \"--weight_decay=0.0\",\n    f\"--warmup_steps={CONFIG['warmup_steps']}\",  # Changed from warmup_ratio\n    f\"--lr_scheduler_type={CONFIG['lr_scheduler_type']}\",\n    \"--logging_steps=10\",\n    f\"--save_steps={CONFIG['save_steps']}\",\n    f\"--save_total_limit={CONFIG['save_total_limit']}\",\n    f\"--model_max_length={CONFIG['model_max_length']}\",\n    \"--gradient_checkpointing=True\",\n    \"--dataloader_num_workers=4\",\n    \"--lazy_preprocess=True\",\n    \"--bf16=True\",\n    \"--report_to=none\",\n    # LoRA\n    \"--lora_enable=True\",\n    f\"--lora_r={CONFIG['lora_r']}\",\n    f\"--lora_alpha={CONFIG['lora_alpha']}\",\n    f\"--lora_dropout={CONFIG['lora_dropout']}\",\n]\n\nif use_tf32:\n    cmd.append(\"--tf32=True\")\nelse:\n    cmd.append(\"--tf32=False\")\n\nprint(f\"Starting training for {CONFIG['max_steps']} steps...\")\nprint(f\"Checkpoints will be saved every {CONFIG['save_steps']} steps\")\n\nenv = os.environ.copy()\nenv[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nprocess = subprocess.Popen(\n    cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    universal_newlines=True,\n    bufsize=1,\n    env=env,\n)\n\nfor line in process.stdout:\n    print(line, end='')\n\nprocess.wait()\nprint(f\"\\nâœ… Training completed with return code: {process.returncode}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Save Model Properly (IMPORTANT!)\n\nThis section saves the trained model with ALL weights including mm_projector."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CHECK TRAINING OUTPUT\n# ============================================\nimport os\n\nOUTPUT_DIR = CONFIG[\"output_dir\"]\nprint(f\"Checking: {OUTPUT_DIR}\\n\")\n\n# Check for checkpoint folders\ncheckpoints = []\nif os.path.exists(OUTPUT_DIR):\n    for item in os.listdir(OUTPUT_DIR):\n        item_path = os.path.join(OUTPUT_DIR, item)\n        if os.path.isdir(item_path) and \"checkpoint\" in item:\n            checkpoints.append(item_path)\n\n# List all files\nall_files = []\nfor root, dirs, files in os.walk(OUTPUT_DIR):\n    for f in files:\n        path = os.path.join(root, f)\n        size = os.path.getsize(path) / 1024 / 1024\n        all_files.append((path, size))\n        print(f\"  {path} ({size:.2f} MB)\")\n\nprint(f\"\\nTotal files: {len(all_files)}\")\nprint(f\"Checkpoints found: {checkpoints}\")\n\n# Check important files\nif len(all_files) == 0:\n    print(\"\\nâš ï¸ OUTPUT DIRECTORY IS EMPTY!\")\n    print(\"ðŸ‘‰ Báº¡n cáº§n cháº¡y CELL TRAINING (cell 19) trÆ°á»›c!\")\n    print(\"ðŸ‘‰ Äáº£m báº£o training hoÃ n thÃ nh vÃ  save checkpoint\")\n    TRAINING_DONE = False\nelse:\n    # Find latest checkpoint or root\n    if checkpoints:\n        LOAD_PATH = sorted(checkpoints)[-1]  # Latest checkpoint\n        print(f\"\\nâœ… Will load from checkpoint: {LOAD_PATH}\")\n    else:\n        LOAD_PATH = OUTPUT_DIR\n        print(f\"\\nâœ… Will load from: {LOAD_PATH}\")\n    TRAINING_DONE = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXTRACT MM_PROJECTOR FROM NON_LORA_TRAINABLES\n# ============================================\nimport torch\nimport os\nimport shutil\n\nOUTPUT_DIR = CONFIG[\"output_dir\"]\nMERGED_DIR = os.path.join(OUTPUT_DIR, \"merged\")\nos.makedirs(MERGED_DIR, exist_ok=True)\n\n# Load non_lora_trainables.bin (contains mm_projector)\nnon_lora_path = os.path.join(OUTPUT_DIR, \"non_lora_trainables.bin\")\n\nif os.path.exists(non_lora_path):\n    print(f\"Loading: {non_lora_path}\")\n    non_lora_weights = torch.load(non_lora_path, map_location=\"cpu\")\n    \n    print(f\"\\nKeys in non_lora_trainables.bin:\")\n    for k in non_lora_weights.keys():\n        print(f\"  {k}: {non_lora_weights[k].shape}\")\n    \n    # Extract mm_projector weights\n    mm_projector_weights = {}\n    for k, v in non_lora_weights.items():\n        if 'mm_projector' in k:\n            # Clean key name\n            clean_key = k.replace('base_model.model.model.', 'model.')\n            clean_key = clean_key.replace('model.model.', 'model.')\n            mm_projector_weights[clean_key] = v\n            print(f\"  âœ… Extracted: {clean_key}\")\n    \n    # Save mm_projector.bin\n    mm_projector_path = os.path.join(MERGED_DIR, \"mm_projector.bin\")\n    torch.save(mm_projector_weights, mm_projector_path)\n    print(f\"\\nâœ… Saved mm_projector to: {mm_projector_path}\")\n    print(f\"   Size: {os.path.getsize(mm_projector_path) / 1024 / 1024:.2f} MB\")\nelse:\n    print(f\"âŒ File not found: {non_lora_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# MERGE LORA + MM_PROJECTOR INTO ONE MODEL\n# ============================================\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load base model\nprint(\"Loading base Qwen3 model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    CONFIG[\"llm_model\"],\n    torch_dtype=torch.float16,\n    device_map=\"cpu\",  # Load to CPU for merging\n    trust_remote_code=True,\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(CONFIG[\"llm_model\"], trust_remote_code=True)\ntokenizer.save_pretrained(MERGED_DIR)\n\n# Load and merge LoRA\nprint(\"Loading LoRA adapter...\")\nmodel = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n\nprint(\"Merging LoRA weights...\")\nmodel = model.merge_and_unload()\n\n# Load mm_projector from non_lora_trainables.bin\nprint(\"\\nLoading mm_projector weights...\")\nnon_lora_path = os.path.join(OUTPUT_DIR, \"non_lora_trainables.bin\")\nnon_lora_weights = torch.load(non_lora_path, map_location=\"cpu\")\n\n# Get model state dict\nstate_dict = model.state_dict()\n\n# Add mm_projector weights to state dict\nmm_count = 0\nfor k, v in non_lora_weights.items():\n    if 'mm_projector' in k:\n        # Clean key: remove prefixes\n        clean_key = k\n        for prefix in ['base_model.model.', 'model.']:\n            if clean_key.startswith(prefix):\n                clean_key = clean_key[len(prefix):]\n        \n        # Add \"model.\" prefix for LlavaQwen3 structure\n        if not clean_key.startswith('model.'):\n            clean_key = 'model.' + clean_key\n        \n        state_dict[clean_key] = v.to(torch.float16)\n        print(f\"  âœ… Added: {clean_key} {v.shape}\")\n        mm_count += 1\n\nprint(f\"\\nAdded {mm_count} mm_projector tensors\")\n\n# Save merged model with mm_projector included\nprint(\"\\nSaving complete model...\")\nmodel.save_pretrained(MERGED_DIR, state_dict=state_dict, safe_serialization=True)\n\nprint(f\"\\nâœ… Model saved to: {MERGED_DIR}\")\nprint(\"   mm_projector is NOW INCLUDED in model.safetensors!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# COPY CONFIG AND VERIFY\n# ============================================\nimport shutil\nimport json\n\n# Copy config.json from training output (has mm_vision_tower settings)\nsrc_config = os.path.join(OUTPUT_DIR, \"config.json\")\ndst_config = os.path.join(MERGED_DIR, \"config.json\")\n\nif os.path.exists(src_config):\n    # Load and update config\n    with open(src_config, 'r') as f:\n        config_data = json.load(f)\n    \n    # Add auto_map for HuggingFace\n    config_data[\"auto_map\"] = {\n        \"AutoConfig\": \"configuration_llava_qwen.LlavaQwen3Config\",\n        \"AutoModelForCausalLM\": \"modeling_llava_qwen.LlavaQwen3ForCausalLM\"\n    }\n    config_data[\"architectures\"] = [\"LlavaQwen3ForCausalLM\"]\n    \n    with open(dst_config, 'w') as f:\n        json.dump(config_data, f, indent=2)\n    \n    print(\"âœ… Updated config.json with auto_map\")\n\n# Verify merged directory\nprint(f\"\\nðŸ“ Files in {MERGED_DIR}:\")\nfor f in sorted(os.listdir(MERGED_DIR)):\n    size = os.path.getsize(os.path.join(MERGED_DIR, f)) / 1024 / 1024\n    print(f\"  {f} ({size:.2f} MB)\")\n\n# Check important files\nprint(\"\\nâœ… Verification:\")\nfor f in [\"model.safetensors\", \"mm_projector.bin\", \"config.json\", \"tokenizer.json\"]:\n    path = os.path.join(MERGED_DIR, f)\n    status = \"âœ…\" if os.path.exists(path) else \"âŒ\"\n    print(f\"  {status} {f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Initialize API\n",
    "api = HfApi(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# Create repo if not exists\n",
    "try:\n",
    "    create_repo(\n",
    "        repo_id=CONFIG[\"hf_repo\"],\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True,\n",
    "        token=CONFIG[\"hf_token\"]\n",
    "    )\n",
    "    print(f\"Repository ready: {CONFIG['hf_repo']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repo exists or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create model card\nmodel_card = f\"\"\"---\nlicense: apache-2.0\nlanguage:\n- vi\n- en\ntags:\n- vision-language-model\n- vlm\n- qwen3\n- fastvlm\n- vietnamese\nbase_model: {CONFIG['llm_model']}\ndatasets:\n- {CONFIG['dataset_name']}\n---\n\n# Belle-VLM: Vietnamese Vision Language Model\n\n## Model Description\n\nBelle-VLM is a Vision Language Model trained for Vietnamese multimodal reasoning tasks.\n\n### Architecture\n- **LLM Backbone**: Qwen3-0.6B\n- **Vision Encoder**: FastViTHD (MobileCLIP)\n- **Projector**: MLP 2-layer (3072 -> 1024)\n\n### Training\n- **Dataset**: 5CD-AI/Viet-multimodal-open-r1-8k-verified\n- **Method**: LoRA fine-tuning\n- **Steps**: {CONFIG['max_steps']}\n- **Learning Rate**: {CONFIG['learning_rate']}\n\n## Usage\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"{CONFIG['hf_repo']}\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"{CONFIG['hf_repo']}\", trust_remote_code=True)\n```\n\n## Training Details\n\n| Parameter | Value |\n|-----------|-------|\n| Base Model | {CONFIG['llm_model']} |\n| Vision Tower | {CONFIG['vision_tower']} |\n| LoRA Rank | {CONFIG['lora_r']} |\n| LoRA Alpha | {CONFIG['lora_alpha']} |\n| Batch Size | {CONFIG['per_device_train_batch_size']} x {CONFIG['gradient_accumulation_steps']} |\n| Max Steps | {CONFIG['max_steps']} |\n\n## License\n\nApache 2.0\n\"\"\"\n\n# Save model card - FIX: use MERGED_DIR not MERGED_PATH\nreadme_path = os.path.join(MERGED_DIR, \"README.md\")\nwith open(readme_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(model_card)\n\nprint(\"âœ… Model card created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# UPLOAD TO HUGGINGFACE (with custom code)\n# ============================================\nfrom huggingface_hub import HfApi\n\napi = HfApi(token=CONFIG[\"hf_token\"])\n\nprint(f\"Uploading to {CONFIG['hf_repo']}...\")\n\n# 1. Upload model files\napi.upload_folder(\n    folder_path=MERGED_DIR,\n    repo_id=CONFIG[\"hf_repo\"],\n    repo_type=\"model\",\n    commit_message=\"Upload Belle-VLM model weights\",\n)\nprint(\"âœ… Model weights uploaded!\")\n\n# 2. Create and upload custom code files for trust_remote_code\nprint(\"\\nCreating custom code files...\")\n\n# configuration_llava_qwen.py\nconfig_code = '''# Configuration for LLaVA Qwen3 model\nfrom transformers import Qwen2Config\ntry:\n    from transformers import Qwen3Config\n    QWEN3_AVAILABLE = True\nexcept ImportError:\n    QWEN3_AVAILABLE = False\n    Qwen3Config = Qwen2Config\n\nclass LlavaQwen3Config(Qwen3Config if QWEN3_AVAILABLE else Qwen2Config):\n    model_type = \"llava_qwen3\"\n    def __init__(self, mm_vision_tower=None, mm_hidden_size=None, mm_projector_type=\"mlp2x_gelu\",\n                 mm_vision_select_layer=-2, mm_vision_select_feature=\"patch\", mm_patch_merge_type=\"flat\",\n                 mm_use_im_start_end=False, mm_use_im_patch_token=False, image_aspect_ratio=\"pad\", **kwargs):\n        super().__init__(**kwargs)\n        self.mm_vision_tower = mm_vision_tower\n        self.mm_hidden_size = mm_hidden_size\n        self.mm_projector_type = mm_projector_type\n        self.mm_vision_select_layer = mm_vision_select_layer\n        self.mm_vision_select_feature = mm_vision_select_feature\n        self.mm_patch_merge_type = mm_patch_merge_type\n        self.mm_use_im_start_end = mm_use_im_start_end\n        self.mm_use_im_patch_token = mm_use_im_patch_token\n        self.image_aspect_ratio = image_aspect_ratio\n'''\n\n# Save and upload config code\nwith open(\"/tmp/configuration_llava_qwen.py\", \"w\") as f:\n    f.write(config_code)\n\napi.upload_file(\n    path_or_fileobj=\"/tmp/configuration_llava_qwen.py\",\n    path_in_repo=\"configuration_llava_qwen.py\",\n    repo_id=CONFIG[\"hf_repo\"],\n)\nprint(\"âœ… Uploaded configuration_llava_qwen.py\")\n\n# modeling_llava_qwen.py (simplified version that loads mm_projector)\nmodeling_code = '''# LLaVA Qwen3 Model\nfrom typing import List, Optional, Tuple, Union\nfrom abc import ABC, abstractmethod\nimport os\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModelForCausalLM, Qwen2Config, Qwen2Model, Qwen2ForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\nfrom huggingface_hub import hf_hub_download\n\ntry:\n    from transformers import Qwen3Config, Qwen3Model, Qwen3ForCausalLM\n    QWEN3_AVAILABLE = True\nexcept ImportError:\n    QWEN3_AVAILABLE = False\n    Qwen3Config, Qwen3Model, Qwen3ForCausalLM = Qwen2Config, Qwen2Model, Qwen2ForCausalLM\n\nfrom .configuration_llava_qwen import LlavaQwen3Config\n\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\n\ndef build_vision_projector(config):\n    projector_type = getattr(config, \"mm_projector_type\", \"mlp2x_gelu\")\n    mm_hidden_size = getattr(config, \"mm_hidden_size\", 3072)\n    hidden_size = config.hidden_size\n    if projector_type == \"mlp2x_gelu\":\n        return nn.Sequential(nn.Linear(mm_hidden_size, hidden_size), nn.GELU(), nn.Linear(hidden_size, hidden_size))\n    return nn.Linear(mm_hidden_size, hidden_size)\n\nclass MobileCLIPVisionTower(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.is_loaded = False\n        self.image_processor = None\n        self.vision_tower = None\n        self.hidden_size = getattr(config, \"mm_hidden_size\", 3072)\n        try:\n            self.image_size = int(getattr(config, \"mm_vision_tower\", \"mobileclip_l_384\").split(\"_\")[-1])\n        except:\n            self.image_size = 384\n\n    def load_model(self, **kwargs):\n        if self.is_loaded:\n            return\n        try:\n            import timm\n            from transformers import CLIPImageProcessor\n            self.vision_tower = timm.create_model(\"fastvit_mci2.apple_mclip\", pretrained=True, num_classes=0)\n            self.vision_tower.eval()\n            self.image_processor = CLIPImageProcessor(size={\"shortest_edge\": self.image_size},\n                crop_size={\"height\": self.image_size, \"width\": self.image_size}, do_normalize=True,\n                image_mean=[0.48145466, 0.4578275, 0.40821073], image_std=[0.26862954, 0.26130258, 0.27577711])\n            self.is_loaded = True\n            print(f\"MobileCLIP loaded!\")\n        except Exception as e:\n            print(f\"Could not load MobileCLIP: {e}\")\n            self.is_loaded = True\n\n    def forward(self, images):\n        if not self.is_loaded:\n            self.load_model()\n        with torch.no_grad():\n            features = self.vision_tower.forward_features(images) if hasattr(self.vision_tower, \"forward_features\") else self.vision_tower(images)\n        if features.dim() == 4:\n            B, C, H, W = features.shape\n            features = features.flatten(2).transpose(1, 2)\n        elif features.dim() == 2:\n            features = features.unsqueeze(1)\n        return features\n\n    def to(self, *args, **kwargs):\n        super().to(*args, **kwargs)\n        if self.vision_tower is not None:\n            self.vision_tower = self.vision_tower.to(*args, **kwargs)\n        return self\n\nclass LlavaMetaModel:\n    def __init__(self, config):\n        super().__init__(config)\n        if hasattr(config, \"mm_vision_tower\") and config.mm_vision_tower:\n            self.vision_tower = MobileCLIPVisionTower(config)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        return getattr(self, \"vision_tower\", None)\n\n    def load_mm_projector(self, model_path):\n        \"\"\"Load mm_projector weights from file or HuggingFace.\"\"\"\n        try:\n            if os.path.isdir(model_path):\n                projector_path = os.path.join(model_path, \"mm_projector.bin\")\n            else:\n                projector_path = hf_hub_download(repo_id=model_path, filename=\"mm_projector.bin\")\n            \n            if os.path.exists(projector_path):\n                weights = torch.load(projector_path, map_location=\"cpu\")\n                # Load weights into mm_projector\n                projector_state = {}\n                for k, v in weights.items():\n                    if \"mm_projector\" in k:\n                        new_key = k.replace(\"model.mm_projector.\", \"\").replace(\"mm_projector.\", \"\")\n                        projector_state[new_key] = v\n                self.mm_projector.load_state_dict(projector_state, strict=False)\n                print(f\"Loaded mm_projector from {projector_path}\")\n        except Exception as e:\n            print(f\"Could not load mm_projector: {e}\")\n\nclass LlavaMetaForCausalLM(ABC):\n    @abstractmethod\n    def get_model(self): pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        vt = self.get_model().get_vision_tower()\n        if not vt.is_loaded:\n            vt.load_model()\n            vt = vt.to(device=images.device, dtype=images.dtype)\n        features = vt(images)\n        return self.get_model().mm_projector(features)\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None):\n        vt = self.get_vision_tower()\n        if vt is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n        \n        image_features = self.encode_images(images) if images.ndim == 4 else self.encode_images(torch.cat([img for img in images], dim=0))\n        if images.ndim == 5 or isinstance(images, list):\n            image_features = [x.flatten(0, 1) for x in torch.split(image_features, [img.shape[0] for img in images], dim=0)]\n\n        _labels, _position_ids, _attention_mask = labels, position_ids, attention_mask\n        attention_mask = torch.ones_like(input_ids, dtype=torch.bool) if attention_mask is None else attention_mask.bool()\n        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device) if position_ids is None else position_ids\n        labels = torch.full_like(input_ids, IGNORE_INDEX) if labels is None else labels\n\n        input_ids = [cur[mask] for cur, mask in zip(input_ids, attention_mask)]\n        labels = [cur[mask] for cur, mask in zip(labels, attention_mask)]\n\n        new_embeds, new_labels = [], []\n        cur_img_idx = 0\n        for batch_idx, cur_ids in enumerate(input_ids):\n            num_imgs = (cur_ids == IMAGE_TOKEN_INDEX).sum()\n            if num_imgs == 0:\n                new_embeds.append(self.get_model().embed_tokens(cur_ids))\n                new_labels.append(labels[batch_idx])\n                cur_img_idx += 1\n                continue\n            \n            img_indices = [-1] + torch.where(cur_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_ids.shape[0]]\n            cur_embeds, cur_labels_new = [], []\n            for i in range(len(img_indices) - 1):\n                cur_embeds.append(self.get_model().embed_tokens(cur_ids[img_indices[i]+1:img_indices[i+1]]))\n                cur_labels_new.append(labels[batch_idx][img_indices[i]+1:img_indices[i+1]])\n                if i < num_imgs:\n                    img_feat = image_features[cur_img_idx] if isinstance(image_features, list) else image_features[cur_img_idx:cur_img_idx+1].squeeze(0)\n                    cur_img_idx += 1\n                    cur_embeds.append(img_feat.to(cur_embeds[-1].dtype))\n                    cur_labels_new.append(torch.full((img_feat.shape[0],), IGNORE_INDEX, device=labels[batch_idx].device, dtype=labels[batch_idx].dtype))\n            new_embeds.append(torch.cat(cur_embeds))\n            new_labels.append(torch.cat(cur_labels_new))\n\n        max_len = max(x.shape[0] for x in new_embeds)\n        batch_size = len(new_embeds)\n        padded_embeds = []\n        padded_labels = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        new_attn = torch.zeros((batch_size, max_len), dtype=torch.bool, device=input_ids[0].device)\n        new_pos = torch.zeros((batch_size, max_len), dtype=torch.long, device=input_ids[0].device)\n\n        for i, (emb, lab) in enumerate(zip(new_embeds, new_labels)):\n            cur_len = emb.shape[0]\n            padded_embeds.append(torch.cat([emb, torch.zeros((max_len - cur_len, emb.shape[1]), dtype=emb.dtype, device=emb.device)], dim=0))\n            padded_labels[i, :cur_len] = lab\n            new_attn[i, :cur_len] = True\n            new_pos[i, :cur_len] = torch.arange(cur_len, device=emb.device)\n\n        return None, new_pos if _position_ids is not None else None, new_attn.to(_attention_mask.dtype) if _attention_mask is not None else None, past_key_values, torch.stack(padded_embeds), padded_labels if _labels is not None else None\n\nclass LlavaQwen3Model(LlavaMetaModel, Qwen3Model):\n    config_class = LlavaQwen3Config\n    def __init__(self, config):\n        super().__init__(config)\n\nclass LlavaQwen3ForCausalLM(Qwen3ForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwen3Config\n    def __init__(self, config):\n        super(Qwen3ForCausalLM, self).__init__(config)\n        self.model = LlavaQwen3Model(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(self, input_ids=None, attention_mask=None, position_ids=None, past_key_values=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, images=None, image_sizes=None, return_dict=None, **kwargs):\n        if inputs_embeds is None:\n            input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n        return super().forward(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, labels=labels, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)\n\n    @torch.no_grad()\n    def generate(self, inputs=None, images=None, image_sizes=None, **kwargs):\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if images is not None:\n            inputs, position_ids, attention_mask, _, inputs_embeds, _ = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\nAutoConfig.register(\"llava_qwen3\", LlavaQwen3Config)\nAutoModelForCausalLM.register(LlavaQwen3Config, LlavaQwen3ForCausalLM)\n'''\n\nwith open(\"/tmp/modeling_llava_qwen.py\", \"w\") as f:\n    f.write(modeling_code)\n\napi.upload_file(\n    path_or_fileobj=\"/tmp/modeling_llava_qwen.py\",\n    path_in_repo=\"modeling_llava_qwen.py\",\n    repo_id=CONFIG[\"hf_repo\"],\n)\nprint(\"âœ… Uploaded modeling_llava_qwen.py\")\n\n# 3. Update config.json with auto_map\nimport json\n\nconfig_path = os.path.join(MERGED_DIR, \"config.json\")\nwith open(config_path, \"r\") as f:\n    config_data = json.load(f)\n\nconfig_data[\"auto_map\"] = {\n    \"AutoConfig\": \"configuration_llava_qwen.LlavaQwen3Config\",\n    \"AutoModelForCausalLM\": \"modeling_llava_qwen.LlavaQwen3ForCausalLM\"\n}\nconfig_data[\"architectures\"] = [\"LlavaQwen3ForCausalLM\"]\n\nwith open(config_path, \"w\") as f:\n    json.dump(config_data, f, indent=2)\n\napi.upload_file(\n    path_or_fileobj=config_path,\n    path_in_repo=\"config.json\",\n    repo_id=CONFIG[\"hf_repo\"],\n)\nprint(\"âœ… Updated and uploaded config.json\")\n\nprint(f\"\\n{'='*50}\")\nprint(f\"âœ… Model uploaded successfully!\")\nprint(f\"ðŸ”— https://huggingface.co/{CONFIG['hf_repo']}\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Test Inference (From HuggingFace)\n\nThis section loads the model from HuggingFace and runs inference."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# INFERENCE FROM HUGGINGFACE MODEL\n# ============================================\n\nimport sys\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, CLIPImageProcessor\n\n# Add ml-fastvlm to path\nsys.path.insert(0, '/kaggle/working/ml-fastvlm-v2')\n\nfrom llava.conversation import conv_templates\nfrom llava.mm_utils import tokenizer_image_token, process_images\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n\n# ============================================\n# CONFIG\n# ============================================\nHF_MODEL_PATH = \"beyoru/Belle-VLM\"  # Your HuggingFace model\n\nprint(f\"Loading model from: {HF_MODEL_PATH}\")\n\n# ============================================\n# Load Tokenizer\n# ============================================\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\n    HF_MODEL_PATH,\n    trust_remote_code=True,\n    use_fast=False\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"âœ… Tokenizer loaded! Vocab size: {tokenizer.vocab_size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# Load Model from HuggingFace\n# ============================================\nfrom llava.model.language_model.llava_qwen import LlavaQwen3ForCausalLM\n\nprint(\"Loading model...\")\nmodel = LlavaQwen3ForCausalLM.from_pretrained(\n    HF_MODEL_PATH,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.eval()\nprint(f\"âœ… Model loaded! Type: {model.config.model_type}\")\n\n# ============================================\n# Load/Initialize Vision Tower & Image Processor\n# ============================================\nprint(\"Setting up vision tower...\")\n\n# Initialize vision tower if needed\nvision_tower = model.get_vision_tower()\n\nif vision_tower is None:\n    print(\"Vision tower not found in model, initializing...\")\n    # Initialize vision tower from config\n    from llava.model.multimodal_encoder.builder import build_vision_tower\n    vision_tower = build_vision_tower(model.config, delay_load=False)\n    model.model.vision_tower = vision_tower\n\n# Load vision tower if not already loaded\nif not vision_tower.is_loaded:\n    print(\"Loading vision tower weights...\")\n    vision_tower.load_model()\n\n# Move to device\nvision_tower = vision_tower.to(device=model.device, dtype=torch.float16)\n\n# Get image processor\nimage_processor = vision_tower.image_processor\n\n# Fallback if image_processor is still None\nif image_processor is None:\n    print(\"âš ï¸ image_processor is None, using default CLIPImageProcessor...\")\n    image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n    # Override with correct image size for mobileclip_l_384\n    image_processor.size = {\"shortest_edge\": 384}\n    image_processor.crop_size = {\"height\": 384, \"width\": 384}\n\nprint(f\"âœ… Vision tower ready!\")\nprint(f\"   Image processor: {type(image_processor).__name__}\")\nprint(f\"   Image size: {getattr(image_processor, 'size', 'unknown')}\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# TEST INFERENCE\n# ============================================\nfrom datasets import load_dataset\nfrom IPython.display import display\n\n# Load test data\nprint(\"Loading test dataset...\")\ntest_dataset = load_dataset(\"5CD-AI/Viet-multimodal-open-r1-8k-verified\", split=\"train\")\n\ntest_image = test_dataset[0]['image']\ntest_question = test_dataset[0]['vi_problem']\nexpected_answer = test_dataset[0]['vi_solution']\n\n# Display\nprint(\"Test image:\")\ndisplay(test_image.resize((300, 300)))\nprint(f\"\\nQuestion: {test_question[:200]}...\")\n\n# Prepare image\nif test_image.mode != 'RGB':\n    test_image = test_image.convert('RGB')\n\nimage_tensor = process_images([test_image], image_processor, model.config)[0]\n\n# Build prompt\nconv = conv_templates[\"qwen_3\"].copy()\nconv.append_message(conv.roles[0], f\"{DEFAULT_IMAGE_TOKEN}\\n{test_question}\")\nconv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\n\n# Tokenize\ninput_ids = tokenizer_image_token(\n    prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n).unsqueeze(0).to(model.device)\n\nprint(f\"\\nInput shape: {input_ids.shape}\")\nprint(\"Generating response...\")\n\n# Generate\nwith torch.inference_mode():\n    output_ids = model.generate(\n        input_ids,\n        images=image_tensor.unsqueeze(0).to(dtype=torch.float16, device=model.device),\n        image_sizes=[test_image.size],\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.8,\n        max_new_tokens=512,\n        use_cache=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n# Decode\noutput = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n\n# Results\nprint(\"=\" * 60)\nprint(\"INFERENCE RESULTS\")\nprint(\"=\" * 60)\nprint(f\"\\nQuestion:\\n{test_question[:300]}...\")\nprint(f\"\\nModel Response:\\n{output}\")\nprint(f\"\\nExpected:\\n{expected_answer[:300]}...\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# HELPER FUNCTION FOR EASY INFERENCE\n# ============================================\n\ndef ask_vlm(image, question, max_tokens=512, temperature=0.7):\n    \"\"\"\n    Simple inference function for Belle-VLM.\n    \n    Args:\n        image: PIL Image or path to image\n        question: Question about the image (Vietnamese or English)\n        max_tokens: Maximum tokens to generate\n        temperature: Sampling temperature\n    \n    Returns:\n        Model response string\n    \"\"\"\n    from PIL import Image as PILImage\n    \n    # Load image if path\n    if isinstance(image, str):\n        image = PILImage.open(image)\n    \n    # Convert to RGB\n    if image.mode != 'RGB':\n        image = image.convert('RGB')\n    \n    # Process image\n    img_tensor = process_images([image], image_processor, model.config)[0]\n    \n    # Build prompt\n    conv = conv_templates[\"qwen_3\"].copy()\n    conv.append_message(conv.roles[0], f\"{DEFAULT_IMAGE_TOKEN}\\n{question}\")\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n    \n    # Tokenize\n    input_ids = tokenizer_image_token(\n        prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n    ).unsqueeze(0).to(model.device)\n    \n    # Generate\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=img_tensor.unsqueeze(0).to(dtype=torch.float16, device=model.device),\n            image_sizes=[image.size],\n            do_sample=True if temperature > 0 else False,\n            temperature=temperature,\n            top_p=0.8,\n            max_new_tokens=max_tokens,\n            use_cache=True,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    return tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n\n\n# Test the helper function\nprint(\"Testing helper function...\")\nresponse = ask_vlm(test_image, \"MÃ´ táº£ hÃ¬nh áº£nh nÃ y.\")\nprint(f\"Response: {response}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\nThe model has been:\n1. âœ… Trained on Vietnamese multimodal dataset\n2. âœ… Merged LoRA weights\n3. âœ… Pushed to HuggingFace\n4. âœ… Tested with inference\n\n### View your model\nðŸ”— https://huggingface.co/beyoru/Belle-VLM\n\n### Quick Usage\n\n```python\n# Use the ask_vlm function\nresponse = ask_vlm(your_image, \"MÃ´ táº£ hÃ¬nh áº£nh nÃ y.\")\nprint(response)\n\n# Or with custom parameters\nresponse = ask_vlm(\n    image=\"path/to/image.jpg\",\n    question=\"Trong hÃ¬nh cÃ³ gÃ¬?\",\n    max_tokens=1024,\n    temperature=0.5\n)\n```"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}